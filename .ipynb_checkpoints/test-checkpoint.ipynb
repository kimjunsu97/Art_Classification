{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "755db786",
   "metadata": {},
   "source": [
    "0. 경로 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 575,
   "id": "e7bdca08",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.getcwd()\n",
    "base_dir = 'D:/Dacon_Art_Classification/Art_Classification'\n",
    "data_dir = \"D:/Dacon_Art_Classification/data\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0a59874c",
   "metadata": {},
   "source": [
    "0. Config Hyperparameter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 576,
   "id": "29e0bb83",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Config():\n",
    "    def __init__(self):\n",
    "        self.image_size = 224\n",
    "        self.epochs = 30\n",
    "        self.learning_rate = 3e-4\n",
    "        self.batch_size = 64\n",
    "        self.seed = 42\n",
    "cfg = Config()\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dec135e6",
   "metadata": {},
   "source": [
    "0. Random Seed 설정"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 577,
   "id": "c04d210c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def seed_everything(seed):\n",
    "    random.seed(seed)\n",
    "    os.environ['PYTHONHASHSEED'] = str(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "\n",
    "seed_everything(cfg.seed) # Seed 고정"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4377adf",
   "metadata": {},
   "source": [
    "1. Dataset 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 578,
   "id": "ac8f9790",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'Diego Velazquez': 0, 'Vincent van Gogh': 1, 'Claude Monet': 2, 'Edgar Degas': 3, 'Hieronymus Bosch': 4, 'Pierre-Auguste Renoir': 5, 'Rene Magritte': 6, 'Michelangelo': 7, 'Peter Paul Rubens': 8, 'Caravaggio': 9, 'Alfred Sisley': 10, 'Edouard Manet': 11, 'Rembrandt': 12, 'Francisco Goya': 13, 'Pablo Picasso': 14, 'Titian': 15, 'Mikhail Vrubel': 16, 'Leonardo da Vinci': 17, 'Kazimir Malevich': 18, 'Andy Warhol': 19, 'Vasiliy Kandinskiy': 20, 'Gustav Klimt': 21, 'Amedeo Modigliani': 22, 'Henri Rousseau': 23, 'Salvador Dali': 24, 'Pieter Bruegel': 25, 'Albrecht Du rer': 26, 'Paul Gauguin': 27, 'Sandro Botticelli': 28, 'Piet Mondrian': 29, 'Eugene Delacroix': 30, 'Paul Klee': 31, 'William Turner': 32, 'Marc Chagall': 33, 'Jan van Eyck': 34, 'Henri Matisse': 35, 'El Greco': 36, 'Gustave Courbet': 37, 'Andrei Rublev': 38, 'Jackson Pollock': 39, 'Edvard Munch': 40, 'Camille Pissarro': 41, 'Raphael': 42, 'Henri de Toulouse-Lautrec': 43, 'Joan Miro': 44, 'Giotto di Bondone': 45, 'Diego Rivera': 46, 'Frida Kahlo': 47, 'Georges Seurat': 48, 'Paul Cezanne': 49}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import DataLoader, Dataset,random_split\n",
    "from torchvision import transforms\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import cv2\n",
    "\n",
    "class CustomDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, state,img_dir, label:pd.DataFrame=None, transform=None):\n",
    "        self.state = state\n",
    "        self.label = label\n",
    "        self.img_dir = img_dir\n",
    "        self.transform = transform\n",
    "    def __len__(self):\n",
    "        return len(self.label)\n",
    "    def __getitem__(self, idx):\n",
    "        # linux일때 '/' 바꿔서 돌리기\n",
    "        if self.state =='train':\n",
    "            img_path = os.path.join(self.img_dir,\"train\",self.label['img_path'][idx].split('/')[-1])\n",
    "            img_label = self.label['artist'][idx]\n",
    "            img = cv2.imread(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)    \n",
    "            img_label = F.one_hot(torch.Tensor([img_label]).to(torch.int64),num_classes = 50)\n",
    "            return img,img_label\n",
    "        else:\n",
    "            img_path = os.path.join(self.img_dir,\"test\",self.label['img_path'][idx].split('/')[-1])\n",
    "            img = cv2.imread(img_path)\n",
    "            #print(img_path)\n",
    "            if self.transform:\n",
    "                img = self.transform(img)\n",
    "            return img\n",
    "train_label = pd.read_csv(f'{data_dir}/train.csv')\n",
    "test_label = pd.read_csv(f'{data_dir}/test.csv')\n",
    "\n",
    "label_dict = dict(zip(train_label['artist'].unique(),range(50)))\n",
    "label_dict_decode = dict(zip(range(50),train_label['artist'].unique()))\n",
    "print(label_dict)\n",
    "\n",
    "train_label['artist'] = [label_dict[i] for i in train_label['artist']]\n",
    "\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.RandomResizedCrop((224,224),scale = (0.25,1),ratio = (0.5,2)),\n",
    "    transforms.RandomHorizontalFlip(),\n",
    "    transforms.RandomVerticalFlip(),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=(0.485, 0.456, 0.406), std=(0.229, 0.224, 0.225)),\n",
    "\n",
    "])\n",
    "\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 626,
   "id": "9afbfde1",
   "metadata": {},
   "outputs": [],
   "source": [
    "Custom_data = CustomDataset('train',data_dir,train_label,train_transform)\n",
    "train_set_size = int(len(Custom_data) * 0.8)\n",
    "valid_set_size = int(len(Custom_data)-train_set_size )\n",
    "\n",
    "train_set, valid_set = random_split(Custom_data, [train_set_size, valid_set_size])\n",
    "\n",
    "train_dataloader = DataLoader(train_set, batch_size=64, shuffle=True)\n",
    "valid_dataloader = DataLoader(valid_set, batch_size=64, shuffle=True)\n",
    "\n",
    "test_data = CustomDataset('test',data_dir,test_label,test_transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=64, shuffle=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44137b35",
   "metadata": {},
   "source": [
    "2. 모델 만들기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 627,
   "id": "1b607bfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torchvision.models import resnet50, ResNet50_Weights\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.backbone = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "        #num_ftrs = self.backbone.fc.in_features\n",
    "        #self.backbone.fc = nn.Sequential()\n",
    "        #self.classifier = nn.Sequential(\n",
    "        #    nn.Linear(num_ftrs,200,bias=True),\n",
    "        #    nn.ReLU(inplace=True),\n",
    "        #    nn.Dropout(0.5,inplace = True),\n",
    "        #    nn.Linear(200, 50, bias=True))\n",
    "        self.classifier = nn.Linear(1000, 50)\n",
    "\n",
    "    def forward(self,x):\n",
    "        x = self.backbone(x)\n",
    "        x = self.classifier(x)\n",
    "        x = torch.nn.functional.sigmoid(x)\n",
    "        #x = torch.nn.functional.softmax(x,dim=0)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 628,
   "id": "01d71385",
   "metadata": {},
   "outputs": [],
   "source": [
    "#from torchvision.models import resnet50, ResNet50_Weights\n",
    "#import torch.nn as nn\n",
    "#model_pretrained = resnet50(weights=ResNet50_Weights.IMAGENET1K_V2)\n",
    "\n",
    "#num_ftrs = model_pretrained.fc.in_features\n",
    "\n",
    "#model_pretrained.fc = nn.Sequential(\n",
    "#    nn.Linear(num_ftrs,200,bias=True),\n",
    "#    nn.ReLU(inplace=True),\n",
    "#    nn.Linear(200, 50),\n",
    "#    nn.ReLU(inplace=True))\n",
    "#print(model_pretrained)\n",
    "#이거 쓰는 법도 공부해보기"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6edd69d2",
   "metadata": {},
   "source": [
    "3. 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "id": "425dbf23",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "from sklearn.metrics import f1_score   \n",
    "\n",
    "def train(model,num_epoch, cirterion, optimizer, train_dataloader, valid_dataloader, scheduler, device): \n",
    "    train_loss_history = []\n",
    "    valid_loss_history = []\n",
    "    best_score = 0\n",
    "    best_model = None\n",
    "    \n",
    "    for epoch in range(num_epoch):\n",
    "        print(\"epoch is :\",epoch)\n",
    "        model.train()\n",
    "        train_loss = []\n",
    "        for i, (images,labels) in enumerate(tqdm(train_dataloader)):\n",
    "            optimizer.zero_grad()\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device) # one hot 인코딩이 아닐시에 tensor 처리 안해도 됨\n",
    "            outputs = model(images)\n",
    "            loss = criterion(outputs, labels.reshape(-1,50).float()) # one hot 인코딩이 아닐시에 reshape 처리 안해도 됨\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            train_loss.append(loss.item())\n",
    "        train_loss_history.append(np.mean(train_loss))\n",
    "        val_loss,val_score = validation(model, criterion, valid_dataloader, device)\n",
    "        valid_loss_history.append(val_loss)\n",
    "        print(f'Epoch [{epoch}], Train Loss : [{np.mean(train_loss):.5f}] Val Loss : [{val_loss:.5f}] Val F1 Score : [{val_score:.5f}]')\n",
    "        #print(f'Epoch [{epoch}], Train Loss : [{np.mean(train_loss):.5f}]')    \n",
    "        if scheduler is not None:\n",
    "            scheduler.step()\n",
    "            \n",
    "        #if best_score < val_score:\n",
    "        #    best_model = model\n",
    "        #    best_score = val_score\n",
    "        \n",
    "    return model, train_loss_history, valid_loss_history\n",
    "def validation(model, criterion, data_loader, device):\n",
    "    model.eval()\n",
    "    model_preds = []\n",
    "    true_labels = []\n",
    "    val_loss = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for img, label in tqdm(data_loader):\n",
    "            img, label = img.to(device), label.to(device).reshape(-1,50).float()\n",
    "            \n",
    "            model_pred = model(img)\n",
    "            \n",
    "            loss = criterion(model_pred,label)\n",
    "            \n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "            model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist() #index 뽑기\n",
    "            true_labels += label.argmax(1).detach().cpu().numpy().tolist()\n",
    "            \n",
    "    val_f1 = competition_metric(true_labels, model_preds)\n",
    "    return np.mean(val_loss), val_f1\n",
    "\n",
    "from sklearn.metrics import f1_score\n",
    "def competition_metric(true, pred):\n",
    "    return f1_score(true, pred, average=\"macro\") \n",
    "        \n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 630,
   "id": "0b17ac34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "backbone.conv1.weight\n",
      "backbone.bn1.weight\n",
      "backbone.bn1.bias\n",
      "backbone.layer1.0.conv1.weight\n",
      "backbone.layer1.0.bn1.weight\n",
      "backbone.layer1.0.bn1.bias\n",
      "backbone.layer1.0.conv2.weight\n",
      "backbone.layer1.0.bn2.weight\n",
      "backbone.layer1.0.bn2.bias\n",
      "backbone.layer1.0.conv3.weight\n",
      "backbone.layer1.0.bn3.weight\n",
      "backbone.layer1.0.bn3.bias\n",
      "backbone.layer1.0.downsample.0.weight\n",
      "backbone.layer1.0.downsample.1.weight\n",
      "backbone.layer1.0.downsample.1.bias\n",
      "backbone.layer1.1.conv1.weight\n",
      "backbone.layer1.1.bn1.weight\n",
      "backbone.layer1.1.bn1.bias\n",
      "backbone.layer1.1.conv2.weight\n",
      "backbone.layer1.1.bn2.weight\n",
      "backbone.layer1.1.bn2.bias\n",
      "backbone.layer1.1.conv3.weight\n",
      "backbone.layer1.1.bn3.weight\n",
      "backbone.layer1.1.bn3.bias\n",
      "backbone.layer1.2.conv1.weight\n",
      "backbone.layer1.2.bn1.weight\n",
      "backbone.layer1.2.bn1.bias\n",
      "backbone.layer1.2.conv2.weight\n",
      "backbone.layer1.2.bn2.weight\n",
      "backbone.layer1.2.bn2.bias\n",
      "backbone.layer1.2.conv3.weight\n",
      "backbone.layer1.2.bn3.weight\n",
      "backbone.layer1.2.bn3.bias\n",
      "backbone.layer2.0.conv1.weight\n",
      "backbone.layer2.0.bn1.weight\n",
      "backbone.layer2.0.bn1.bias\n",
      "backbone.layer2.0.conv2.weight\n",
      "backbone.layer2.0.bn2.weight\n",
      "backbone.layer2.0.bn2.bias\n",
      "backbone.layer2.0.conv3.weight\n",
      "backbone.layer2.0.bn3.weight\n",
      "backbone.layer2.0.bn3.bias\n",
      "backbone.layer2.0.downsample.0.weight\n",
      "backbone.layer2.0.downsample.1.weight\n",
      "backbone.layer2.0.downsample.1.bias\n",
      "backbone.layer2.1.conv1.weight\n",
      "backbone.layer2.1.bn1.weight\n",
      "backbone.layer2.1.bn1.bias\n",
      "backbone.layer2.1.conv2.weight\n",
      "backbone.layer2.1.bn2.weight\n",
      "backbone.layer2.1.bn2.bias\n",
      "backbone.layer2.1.conv3.weight\n",
      "backbone.layer2.1.bn3.weight\n",
      "backbone.layer2.1.bn3.bias\n",
      "backbone.layer2.2.conv1.weight\n",
      "backbone.layer2.2.bn1.weight\n",
      "backbone.layer2.2.bn1.bias\n",
      "backbone.layer2.2.conv2.weight\n",
      "backbone.layer2.2.bn2.weight\n",
      "backbone.layer2.2.bn2.bias\n",
      "backbone.layer2.2.conv3.weight\n",
      "backbone.layer2.2.bn3.weight\n",
      "backbone.layer2.2.bn3.bias\n",
      "backbone.layer2.3.conv1.weight\n",
      "backbone.layer2.3.bn1.weight\n",
      "backbone.layer2.3.bn1.bias\n",
      "backbone.layer2.3.conv2.weight\n",
      "backbone.layer2.3.bn2.weight\n",
      "backbone.layer2.3.bn2.bias\n",
      "backbone.layer2.3.conv3.weight\n",
      "backbone.layer2.3.bn3.weight\n",
      "backbone.layer2.3.bn3.bias\n",
      "backbone.layer3.0.conv1.weight\n",
      "backbone.layer3.0.bn1.weight\n",
      "backbone.layer3.0.bn1.bias\n",
      "backbone.layer3.0.conv2.weight\n",
      "backbone.layer3.0.bn2.weight\n",
      "backbone.layer3.0.bn2.bias\n",
      "backbone.layer3.0.conv3.weight\n",
      "backbone.layer3.0.bn3.weight\n",
      "backbone.layer3.0.bn3.bias\n",
      "backbone.layer3.0.downsample.0.weight\n",
      "backbone.layer3.0.downsample.1.weight\n",
      "backbone.layer3.0.downsample.1.bias\n",
      "backbone.layer3.1.conv1.weight\n",
      "backbone.layer3.1.bn1.weight\n",
      "backbone.layer3.1.bn1.bias\n",
      "backbone.layer3.1.conv2.weight\n",
      "backbone.layer3.1.bn2.weight\n",
      "backbone.layer3.1.bn2.bias\n",
      "backbone.layer3.1.conv3.weight\n",
      "backbone.layer3.1.bn3.weight\n",
      "backbone.layer3.1.bn3.bias\n",
      "backbone.layer3.2.conv1.weight\n",
      "backbone.layer3.2.bn1.weight\n",
      "backbone.layer3.2.bn1.bias\n",
      "backbone.layer3.2.conv2.weight\n",
      "backbone.layer3.2.bn2.weight\n",
      "backbone.layer3.2.bn2.bias\n",
      "backbone.layer3.2.conv3.weight\n",
      "backbone.layer3.2.bn3.weight\n",
      "backbone.layer3.2.bn3.bias\n",
      "backbone.layer3.3.conv1.weight\n",
      "backbone.layer3.3.bn1.weight\n",
      "backbone.layer3.3.bn1.bias\n",
      "backbone.layer3.3.conv2.weight\n",
      "backbone.layer3.3.bn2.weight\n",
      "backbone.layer3.3.bn2.bias\n",
      "backbone.layer3.3.conv3.weight\n",
      "backbone.layer3.3.bn3.weight\n",
      "backbone.layer3.3.bn3.bias\n",
      "backbone.layer3.4.conv1.weight\n",
      "backbone.layer3.4.bn1.weight\n",
      "backbone.layer3.4.bn1.bias\n",
      "backbone.layer3.4.conv2.weight\n",
      "backbone.layer3.4.bn2.weight\n",
      "backbone.layer3.4.bn2.bias\n",
      "backbone.layer3.4.conv3.weight\n",
      "backbone.layer3.4.bn3.weight\n",
      "backbone.layer3.4.bn3.bias\n",
      "backbone.layer3.5.conv1.weight\n",
      "backbone.layer3.5.bn1.weight\n",
      "backbone.layer3.5.bn1.bias\n",
      "backbone.layer3.5.conv2.weight\n",
      "backbone.layer3.5.bn2.weight\n",
      "backbone.layer3.5.bn2.bias\n",
      "backbone.layer3.5.conv3.weight\n",
      "backbone.layer3.5.bn3.weight\n",
      "backbone.layer3.5.bn3.bias\n",
      "backbone.layer4.0.conv1.weight\n",
      "backbone.layer4.0.bn1.weight\n",
      "backbone.layer4.0.bn1.bias\n",
      "backbone.layer4.0.conv2.weight\n",
      "backbone.layer4.0.bn2.weight\n",
      "backbone.layer4.0.bn2.bias\n",
      "backbone.layer4.0.conv3.weight\n",
      "backbone.layer4.0.bn3.weight\n",
      "backbone.layer4.0.bn3.bias\n",
      "backbone.layer4.0.downsample.0.weight\n",
      "backbone.layer4.0.downsample.1.weight\n",
      "backbone.layer4.0.downsample.1.bias\n",
      "backbone.layer4.1.conv1.weight\n",
      "backbone.layer4.1.bn1.weight\n",
      "backbone.layer4.1.bn1.bias\n",
      "backbone.layer4.1.conv2.weight\n",
      "backbone.layer4.1.bn2.weight\n",
      "backbone.layer4.1.bn2.bias\n",
      "backbone.layer4.1.conv3.weight\n",
      "backbone.layer4.1.bn3.weight\n",
      "backbone.layer4.1.bn3.bias\n",
      "backbone.layer4.2.conv1.weight\n",
      "backbone.layer4.2.bn1.weight\n",
      "backbone.layer4.2.bn1.bias\n",
      "backbone.layer4.2.conv2.weight\n",
      "backbone.layer4.2.bn2.weight\n",
      "backbone.layer4.2.bn2.bias\n",
      "backbone.layer4.2.conv3.weight\n",
      "backbone.layer4.2.bn3.weight\n",
      "backbone.layer4.2.bn3.bias\n",
      "backbone.fc.weight\n",
      "backbone.fc.bias\n",
      "classifier.weight\n",
      "classifier.bias\n"
     ]
    }
   ],
   "source": [
    "model = ResNet()\n",
    "for name, param in model.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "841f71e0",
   "metadata": {},
   "source": [
    "Freezing 학습시키기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 633,
   "id": "1ddc9e20",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:10<00:00,  1.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:17<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.93384] Val Loss : [0.90774] Val F1 Score : [0.02557]\n",
      "epoch is : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:08<00:00,  1.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:17<00:00,  1.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.88310] Val Loss : [0.86458] Val F1 Score : [0.04009]\n",
      "epoch is : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:08<00:00,  1.08it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:17<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.84730] Val Loss : [0.83259] Val F1 Score : [0.04471]\n",
      "epoch is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:10<00:00,  1.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:17<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.82187] Val Loss : [0.81477] Val F1 Score : [0.06154]\n",
      "epoch is : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:09<00:00,  1.06it/s]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:17<00:00,  1.10it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.80381] Val Loss : [0.80152] Val F1 Score : [0.07006]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "from sklearn.metrics import f1_score   \n",
    "\n",
    "model = ResNet()\n",
    "for name, param in model.named_parameters():\n",
    "    if 'backbone' in name:\n",
    "        param.requires_grad = False\n",
    "    elif 'fc' in name:\n",
    "        param.requires_grad = True\n",
    "        \n",
    "model.to(device)\n",
    "#criterion = nn.CrossEntropyLoss()\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = None\n",
    "infer_model,train_history, valid_history = train(model, 5, criterion, optimizer, train_dataloader, valid_dataloader, scheduler, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 642,
   "id": "b829fc74",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Input \u001b[1;32mIn [642]\u001b[0m, in \u001b[0;36m<cell line: 9>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;66;03m#for name, param in model.named_parameters():\u001b[39;00m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;66;03m#    if 'backbone' in name:\u001b[39;00m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m#        param.requires_grad = False\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;66;03m#    elif 'fc' in name:\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#        param.requires_grad = True\u001b[39;00m\n\u001b[0;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtorchsummary\u001b[39;00m\n\u001b[1;32m----> 9\u001b[0m \u001b[43mtorchsummary\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msummary\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtest_model\u001b[49m\u001b[43m,\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m224\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torchsummary\\torchsummary.py:72\u001b[0m, in \u001b[0;36msummary\u001b[1;34m(model, input_size, batch_size, device)\u001b[0m\n\u001b[0;32m     68\u001b[0m model\u001b[38;5;241m.\u001b[39mapply(register_hook)\n\u001b[0;32m     70\u001b[0m \u001b[38;5;66;03m# make a forward pass\u001b[39;00m\n\u001b[0;32m     71\u001b[0m \u001b[38;5;66;03m# print(x.shape)\u001b[39;00m\n\u001b[1;32m---> 72\u001b[0m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     74\u001b[0m \u001b[38;5;66;03m# remove these hooks\u001b[39;00m\n\u001b[0;32m     75\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m h \u001b[38;5;129;01min\u001b[39;00m hooks:\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Input \u001b[1;32mIn [627]\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m     18\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m,x):\n\u001b[1;32m---> 19\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackbone\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     20\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mclassifier(x)\n\u001b[0;32m     21\u001b[0m     x \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mnn\u001b[38;5;241m.\u001b[39mfunctional\u001b[38;5;241m.\u001b[39msigmoid(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torchvision\\models\\resnet.py:285\u001b[0m, in \u001b[0;36mResNet.forward\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    284\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 285\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_forward_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torchvision\\models\\resnet.py:268\u001b[0m, in \u001b[0;36mResNet._forward_impl\u001b[1;34m(self, x)\u001b[0m\n\u001b[0;32m    266\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_forward_impl\u001b[39m(\u001b[38;5;28mself\u001b[39m, x: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m    267\u001b[0m     \u001b[38;5;66;03m# See note [TorchScript super()]\u001b[39;00m\n\u001b[1;32m--> 268\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv1\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    269\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mbn1(x)\n\u001b[0;32m    270\u001b[0m     x \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrelu(x)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\modules\\module.py:1148\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1145\u001b[0m     bw_hook \u001b[38;5;241m=\u001b[39m hooks\u001b[38;5;241m.\u001b[39mBackwardHook(\u001b[38;5;28mself\u001b[39m, full_backward_hooks)\n\u001b[0;32m   1146\u001b[0m     \u001b[38;5;28minput\u001b[39m \u001b[38;5;241m=\u001b[39m bw_hook\u001b[38;5;241m.\u001b[39msetup_input_hook(\u001b[38;5;28minput\u001b[39m)\n\u001b[1;32m-> 1148\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1149\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks:\n\u001b[0;32m   1150\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m hook \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;241m*\u001b[39m_global_forward_hooks\u001b[38;5;241m.\u001b[39mvalues(), \u001b[38;5;241m*\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks\u001b[38;5;241m.\u001b[39mvalues()):\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:457\u001b[0m, in \u001b[0;36mConv2d.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    456\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[1;32m--> 457\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_conv_forward\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\modules\\conv.py:453\u001b[0m, in \u001b[0;36mConv2d._conv_forward\u001b[1;34m(self, input, weight, bias)\u001b[0m\n\u001b[0;32m    449\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode \u001b[38;5;241m!=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mzeros\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[0;32m    450\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m F\u001b[38;5;241m.\u001b[39mconv2d(F\u001b[38;5;241m.\u001b[39mpad(\u001b[38;5;28minput\u001b[39m, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_reversed_padding_repeated_twice, mode\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mpadding_mode),\n\u001b[0;32m    451\u001b[0m                     weight, bias, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstride,\n\u001b[0;32m    452\u001b[0m                     _pair(\u001b[38;5;241m0\u001b[39m), \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdilation, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgroups)\n\u001b[1;32m--> 453\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconv2d\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbias\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstride\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    454\u001b[0m \u001b[43m                \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpadding\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdilation\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgroups\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[1;31mRuntimeError\u001b[0m: Input type (torch.cuda.FloatTensor) and weight type (torch.FloatTensor) should be the same"
     ]
    }
   ],
   "source": [
    "test_model = ResNet()\n",
    "#for name, param in model.named_parameters():\n",
    "#    if 'backbone' in name:\n",
    "#        param.requires_grad = False\n",
    "#    elif 'fc' in name:\n",
    "#        param.requires_grad = True\n",
    "        \n",
    "import torchsummary\n",
    "torchsummary.summary(test_model,(3,224,224))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 634,
   "id": "f100607b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch is : 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:15<00:00,  1.02s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:17<00:00,  1.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [0], Train Loss : [0.69586] Val Loss : [0.69317] Val F1 Score : [0.02816]\n",
      "epoch is : 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:16<00:00,  1.04s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:17<00:00,  1.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1], Train Loss : [0.69316] Val Loss : [0.69316] Val F1 Score : [0.02538]\n",
      "epoch is : 2\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:17<00:00,  1.05s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:18<00:00,  1.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2], Train Loss : [0.69316] Val Loss : [0.69316] Val F1 Score : [0.02706]\n",
      "epoch is : 3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:18<00:00,  1.06s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:18<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [3], Train Loss : [0.69315] Val Loss : [0.69315] Val F1 Score : [0.02109]\n",
      "epoch is : 4\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:19<00:00,  1.07s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:18<00:00,  1.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [4], Train Loss : [0.69315] Val Loss : [0.69315] Val F1 Score : [0.03202]\n",
      "epoch is : 5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:20<00:00,  1.08s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:18<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [5], Train Loss : [0.69315] Val Loss : [0.69315] Val F1 Score : [0.02972]\n",
      "epoch is : 6\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:23<00:00,  1.12s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:18<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [6], Train Loss : [0.69315] Val Loss : [0.69315] Val F1 Score : [0.03387]\n",
      "epoch is : 7\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:22<00:00,  1.11s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:19<00:00,  1.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [7], Train Loss : [0.69315] Val Loss : [0.69315] Val F1 Score : [0.03169]\n",
      "epoch is : 8\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:25<00:00,  1.15s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:18<00:00,  1.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [8], Train Loss : [0.69315] Val Loss : [0.69315] Val F1 Score : [0.02284]\n",
      "epoch is : 9\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|                                                                                           | 0/74 [00:00<?, ?it/s]C:\\Users\\kimju\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\nn\\functional.py:1960: UserWarning: nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\n",
      "  warnings.warn(\"nn.functional.sigmoid is deprecated. Use torch.sigmoid instead.\")\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 74/74 [01:20<00:00,  1.09s/it]\n",
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:18<00:00,  1.03it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [9], Train Loss : [0.69315] Val Loss : [0.69315] Val F1 Score : [0.03472]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "for name, param in infer_model.named_parameters():\n",
    "    param.requires_grad = True\n",
    "criterion = nn.BCEWithLogitsLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.0001)\n",
    "scheduler = None\n",
    "infer_model,train_history, valid_history = train(infer_model, 10, criterion, optimizer, train_dataloader, valid_dataloader, scheduler, device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 635,
   "id": "1d29d334",
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[<matplotlib.lines.Line2D at 0x20fa009cb50>]"
      ]
     },
     "execution_count": 635,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.plot(train_history)\n",
    "plt.plot(valid_history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 481,
   "id": "3b896b1f",
   "metadata": {},
   "outputs": [
    {
     "ename": "PicklingError",
     "evalue": "Can't pickle <class '__main__.ResNet'>: it's not the same object as __main__.ResNet",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mPicklingError\u001b[0m                             Traceback (most recent call last)",
      "Input \u001b[1;32mIn [481]\u001b[0m, in \u001b[0;36m<cell line: 1>\u001b[1;34m()\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[43mresnet\u001b[49m\u001b[43m,\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mpretrained_model.pt\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\serialization.py:379\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[0;32m    378\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(opened_file) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 379\u001b[0m         \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    380\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[0;32m    381\u001b[0m _legacy_save(obj, opened_file, pickle_module, pickle_protocol)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\torch\\serialization.py:589\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    587\u001b[0m pickler \u001b[38;5;241m=\u001b[39m pickle_module\u001b[38;5;241m.\u001b[39mPickler(data_buf, protocol\u001b[38;5;241m=\u001b[39mpickle_protocol)\n\u001b[0;32m    588\u001b[0m pickler\u001b[38;5;241m.\u001b[39mpersistent_id \u001b[38;5;241m=\u001b[39m persistent_id\n\u001b[1;32m--> 589\u001b[0m \u001b[43mpickler\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdump\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    590\u001b[0m data_value \u001b[38;5;241m=\u001b[39m data_buf\u001b[38;5;241m.\u001b[39mgetvalue()\n\u001b[0;32m    591\u001b[0m zip_file\u001b[38;5;241m.\u001b[39mwrite_record(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdata.pkl\u001b[39m\u001b[38;5;124m'\u001b[39m, data_value, \u001b[38;5;28mlen\u001b[39m(data_value))\n",
      "\u001b[1;31mPicklingError\u001b[0m: Can't pickle <class '__main__.ResNet'>: it's not the same object as __main__.ResNet"
     ]
    }
   ],
   "source": [
    "torch.save(infer_model,\"pretrained_model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 552,
   "id": "0a4ff66d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████████████████████████████| 19/19 [00:15<00:00,  1.22it/s]\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "test_data = CustomDataset('test',data_dir,test_label,test_transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=True)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "#valid_model = torch.load('model.pt')\n",
    "valid_model = infer_model\n",
    "valid_model.eval()\n",
    "out_list = []\n",
    "label_list= []\n",
    "with torch.no_grad():\n",
    "    for i,(image,label) in enumerate(tqdm(valid_dataloader)):\n",
    "        image = image.to(device)\n",
    "        out = valid_model(image)\n",
    "        out_list.append(out)\n",
    "        label_list.append(label)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 555,
   "id": "884bad03",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "64\n",
      "1183\n"
     ]
    }
   ],
   "source": [
    "print(len(label_list[0]))\n",
    "k = [np.argmax(i,axis=2).flatten() for i in label_list]\n",
    "final_label = []\n",
    "for i in k:\n",
    "    final_label+=i\n",
    "final_label = [i.item() for i in final_label]\n",
    "print(len(final_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 554,
   "id": "a550ae9c",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [554]\u001b[0m, in \u001b[0;36m<cell line: 2>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i \u001b[38;5;129;01min\u001b[39;00m label_list:\n\u001b[0;32m      3\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m j \u001b[38;5;129;01min\u001b[39;00m i:\n\u001b[1;32m----> 4\u001b[0m         final_label\u001b[38;5;241m.\u001b[39mappend(\u001b[43mj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;28mlen\u001b[39m(final_label))\n",
      "\u001b[1;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "source": [
    "final_label = []\n",
    "for i in label_list:\n",
    "    for j in i:\n",
    "        final_label.append(j.item())\n",
    "print(len(final_label))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 556,
   "id": "68cf9d5d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[10, 14, 5, 25, 20, 15, 10, 3, 48, 40, 24, 21, 5, 21, 6, 5, 2, 1, 14, 3, 28, 20, 35, 6, 35, 21, 19, 45, 1, 5, 33, 12, 41, 38, 18, 27, 35, 33, 13, 26, 34, 6, 22, 15, 36, 15, 29, 3, 3, 28, 1, 5, 1, 47, 3, 34, 20, 1, 26, 35, 44, 44, 45, 46, 0, 35, 28, 12, 1, 14, 3, 1, 4, 3, 11, 1, 49, 8, 24, 32, 26, 16, 31, 4, 6, 19, 13, 46, 40, 8, 3, 26, 10, 38, 14, 13, 1, 22, 3, 3, 39, 5, 12, 1, 24, 17, 14, 15, 49, 10, 1, 3, 47, 27, 44, 25, 14, 0, 4, 25, 10, 25, 3, 20, 27, 13, 1, 18, 1, 1, 1, 0, 35, 3, 17, 16, 46, 3, 26, 10, 3, 41, 1, 19, 7, 15, 33, 14, 1, 20, 19, 21, 41, 20, 33, 41, 12, 10, 3, 18, 27, 4, 44, 37, 13, 1, 22, 1, 13, 18, 0, 20, 4, 16, 10, 8, 8, 24, 13, 1, 5, 6, 27, 3, 23, 14, 29, 1, 6, 17, 43, 1, 3, 47, 19, 4, 3, 26, 1, 1, 21, 13, 44, 28, 26, 37, 3, 3, 36, 33, 35, 10, 25, 26, 3, 31, 18, 31, 17, 38, 15, 27, 1, 35, 27, 1, 49, 3, 21, 25, 1, 35, 1, 17, 27, 31, 3, 27, 5, 13, 35, 15, 43, 4, 31, 5, 36, 47, 25, 14, 3, 22, 6, 3, 5, 14, 7, 42, 14, 3, 8, 35, 1, 46, 1, 13, 8, 3, 26, 3, 6, 15, 24, 42, 27, 3, 29, 13, 0, 44, 41, 5, 12, 14, 27, 10, 6, 1, 35, 5, 5, 26, 15, 5, 15, 13, 2, 3, 1, 28, 1, 4, 3, 18, 27, 21, 28, 17, 31, 3, 36, 11, 15, 23, 31, 14, 14, 35, 33, 1, 31, 6, 38, 34, 6, 12, 15, 1, 28, 13, 11, 5, 35, 26, 31, 1, 26, 27, 6, 47, 0, 3, 3, 10, 38, 14, 1, 36, 3, 25, 18, 47, 45, 18, 49, 1, 20, 33, 11, 15, 4, 14, 26, 47, 1, 16, 39, 13, 20, 3, 5, 20, 9, 19, 4, 35, 24, 28, 1, 6, 33, 38, 28, 45, 28, 21, 35, 27, 44, 3, 22, 17, 41, 24, 28, 1, 43, 44, 44, 1, 5, 5, 12, 32, 3, 1, 10, 41, 27, 25, 44, 3, 33, 42, 22, 3, 26, 3, 33, 21, 5, 15, 14, 0, 14, 15, 41, 5, 14, 34, 21, 16, 27, 3, 26, 41, 26, 12, 12, 1, 15, 17, 18, 2, 1, 20, 43, 10, 12, 1, 3, 31, 26, 3, 16, 4, 33, 1, 21, 3, 27, 5, 18, 31, 38, 22, 5, 44, 27, 14, 19, 14, 3, 4, 29, 1, 4, 4, 1, 38, 3, 27, 5, 13, 14, 16, 29, 14, 28, 1, 25, 1, 14, 10, 14, 35, 15, 3, 4, 1, 34, 6, 23, 26, 23, 3, 3, 44, 19, 10, 26, 41, 1, 12, 4, 5, 13, 10, 2, 13, 24, 19, 27, 2, 31, 3, 15, 3, 3, 0, 12, 1, 3, 13, 27, 31, 15, 6, 12, 16, 10, 10, 3, 42, 14, 41, 28, 15, 45, 13, 10, 45, 36, 5, 26, 17, 15, 46, 34, 26, 3, 1, 20, 12, 5, 49, 35, 3, 16, 24, 38, 10, 5, 13, 8, 0, 32, 14, 19, 33, 12, 3, 10, 43, 25, 1, 5, 21, 38, 12, 27, 17, 14, 17, 28, 3, 47, 1, 1, 3, 14, 44, 10, 1, 13, 27, 44, 4, 48, 5, 1, 19, 3, 45, 0, 19, 3, 1, 6, 3, 38, 39, 33, 3, 40, 32, 24, 5, 10, 1, 17, 24, 26, 3, 22, 15, 15, 15, 14, 14, 3, 15, 13, 31, 29, 10, 17, 31, 17, 4, 36, 1, 48, 15, 27, 26, 5, 6, 35, 1, 41, 1, 38, 1, 12, 3, 33, 31, 13, 26, 44, 14, 18, 38, 5, 26, 3, 26, 6, 38, 33, 31, 13, 1, 12, 26, 47, 22, 26, 27, 14, 14, 33, 1, 35, 13, 38, 1, 13, 3, 3, 15, 6, 13, 47, 0, 14, 3, 6, 36, 15, 16, 2, 33, 25, 27, 1, 36, 3, 24, 12, 1, 1, 4, 25, 28, 28, 26, 33, 26, 15, 12, 14, 1, 41, 42, 46, 15, 5, 29, 35, 28, 4, 22, 12, 21, 33, 0, 24, 10, 0, 20, 47, 22, 17, 28, 1, 8, 19, 16, 16, 14, 47, 33, 28, 26, 1, 16, 10, 14, 6, 32, 41, 27, 33, 23, 26, 6, 4, 1, 15, 27, 5, 27, 33, 3, 5, 13, 3, 45, 17, 1, 4, 3, 15, 35, 28, 3, 14, 1, 26, 12, 1, 19, 1, 15, 1, 1, 3, 1, 22, 47, 17, 49, 17, 1, 14, 40, 31, 47, 22, 1, 14, 26, 33, 1, 11, 16, 15, 26, 37, 14, 3, 12, 6, 12, 28, 6, 13, 1, 47, 17, 14, 43, 1, 26, 15, 12, 27, 15, 3, 42, 26, 40, 27, 14, 27, 2, 44, 34, 27, 12, 5, 45, 1, 12, 1, 0, 32, 3, 26, 0, 3, 12, 44, 20, 15, 15, 3, 27, 25, 15, 15, 33, 33, 25, 26, 3, 47, 14, 23, 27, 33, 26, 7, 7, 13, 19, 14, 1, 28, 42, 13, 5, 35, 26, 21, 1, 15, 21, 6, 27, 26, 0, 43, 14, 1, 16, 5, 41, 0, 3, 13, 28, 8, 3, 31, 45, 15, 13, 16, 12, 22, 36, 12, 6, 31, 3, 36, 1, 44, 27, 3, 10, 27, 15, 13, 34, 3, 15, 14, 6, 41, 31, 33, 17, 3, 17, 20, 26, 23, 21, 26, 18, 21, 18, 8, 12, 12, 35, 3, 1, 7, 12, 3, 46, 17, 14, 44, 8, 42, 15, 26, 3, 14, 27, 6, 3, 37, 31, 3, 35, 5, 25, 33, 18, 5, 24, 35, 24, 41, 1, 1, 4, 1, 13, 14, 1, 45, 12, 5, 45, 33, 33, 13, 3, 2, 38, 13, 19, 5, 13, 15, 6, 21, 13, 1, 22, 43, 45, 5, 28, 13, 47, 12, 3, 6, 13, 17, 24, 49, 3, 27, 12, 46, 5, 1, 15, 3, 5, 27, 1, 3, 14, 3, 31, 2, 1, 33, 3, 32, 3, 10, 14, 28, 48, 14, 5, 3, 26, 3, 14, 14, 45, 15, 3, 0, 33, 14, 4, 4, 8, 31, 6, 26, 5, 8, 40, 3, 14, 33, 1, 3, 16, 3, 22, 3, 15, 13, 40, 17, 46, 24, 33, 28, 34, 27, 41, 5, 1, 5, 14, 34, 6, 47, 1, 6, 8, 20, 45, 3, 21, 31, 3, 18, 43, 1, 45, 8, 14, 30, 19, 26, 41, 12, 1, 0, 3, 3, 10, 30, 3, 10, 6, 26, 1, 3, 31, 27, 48, 23, 14, 38, 18, 25, 1, 14, 24, 38, 11, 26, 1, 3, 1, 28, 12, 27, 1, 26, 43, 10, 33, 40, 1, 34, 1, 27, 1, 18, 4, 33, 33, 14]\n"
     ]
    }
   ],
   "source": [
    "final_class = [(i.argmax(axis=1).to('cpu')) for i in out_list]\n",
    "final_output = []\n",
    "for i in final_class:\n",
    "    for j in i:\n",
    "        final_output.append(j.item())\n",
    "print(final_output)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 557,
   "id": "7375aa4b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import f1_score\n",
    "def competition_metric(true, pred):\n",
    "    return f1_score(true, pred, average=\"macro\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 558,
   "id": "9b76f05b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.698742308813219\n"
     ]
    }
   ],
   "source": [
    "score = competition_metric(final_label,final_output)\n",
    "print(score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 522,
   "id": "8388f397",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Found input variables with inconsistent numbers of samples: [0, 1183]",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Input \u001b[1;32mIn [522]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m get_ipython()\u001b[38;5;241m.\u001b[39mrun_line_magic(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmatplotlib\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minline\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msklearn\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m cm \u001b[38;5;241m=\u001b[39m \u001b[43msklearn\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mconfusion_matrix\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_label\u001b[49m\u001b[43m,\u001b[49m\u001b[43mfinal_output\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m sklearn\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mConfusionMatrixDisplay(cm)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\sklearn\\metrics\\_classification.py:307\u001b[0m, in \u001b[0;36mconfusion_matrix\u001b[1;34m(y_true, y_pred, labels, sample_weight, normalize)\u001b[0m\n\u001b[0;32m    222\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mconfusion_matrix\u001b[39m(\n\u001b[0;32m    223\u001b[0m     y_true, y_pred, \u001b[38;5;241m*\u001b[39m, labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, sample_weight\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, normalize\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    224\u001b[0m ):\n\u001b[0;32m    225\u001b[0m     \u001b[38;5;124;03m\"\"\"Compute confusion matrix to evaluate the accuracy of a classification.\u001b[39;00m\n\u001b[0;32m    226\u001b[0m \n\u001b[0;32m    227\u001b[0m \u001b[38;5;124;03m    By definition a confusion matrix :math:`C` is such that :math:`C_{i, j}`\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    305\u001b[0m \u001b[38;5;124;03m    (0, 2, 1, 1)\u001b[39;00m\n\u001b[0;32m    306\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m--> 307\u001b[0m     y_type, y_true, y_pred \u001b[38;5;241m=\u001b[39m \u001b[43m_check_targets\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    308\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m y_type \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbinary\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmulticlass\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[0;32m    309\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;124m is not supported\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;241m%\u001b[39m y_type)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\sklearn\\metrics\\_classification.py:84\u001b[0m, in \u001b[0;36m_check_targets\u001b[1;34m(y_true, y_pred)\u001b[0m\n\u001b[0;32m     57\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_check_targets\u001b[39m(y_true, y_pred):\n\u001b[0;32m     58\u001b[0m     \u001b[38;5;124;03m\"\"\"Check that y_true and y_pred belong to the same classification task.\u001b[39;00m\n\u001b[0;32m     59\u001b[0m \n\u001b[0;32m     60\u001b[0m \u001b[38;5;124;03m    This converts multiclass or binary types to a common shape, and raises a\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[38;5;124;03m    y_pred : array or indicator matrix\u001b[39;00m\n\u001b[0;32m     83\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[1;32m---> 84\u001b[0m     \u001b[43mcheck_consistent_length\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_true\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     85\u001b[0m     type_true \u001b[38;5;241m=\u001b[39m type_of_target(y_true)\n\u001b[0;32m     86\u001b[0m     type_pred \u001b[38;5;241m=\u001b[39m type_of_target(y_pred)\n",
      "File \u001b[1;32m~\\anaconda3\\envs\\dacon\\lib\\site-packages\\sklearn\\utils\\validation.py:332\u001b[0m, in \u001b[0;36mcheck_consistent_length\u001b[1;34m(*arrays)\u001b[0m\n\u001b[0;32m    330\u001b[0m uniques \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39munique(lengths)\n\u001b[0;32m    331\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(uniques) \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m--> 332\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[0;32m    333\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFound input variables with inconsistent numbers of samples: \u001b[39m\u001b[38;5;132;01m%r\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m    334\u001b[0m         \u001b[38;5;241m%\u001b[39m [\u001b[38;5;28mint\u001b[39m(l) \u001b[38;5;28;01mfor\u001b[39;00m l \u001b[38;5;129;01min\u001b[39;00m lengths]\n\u001b[0;32m    335\u001b[0m     )\n",
      "\u001b[1;31mValueError\u001b[0m: Found input variables with inconsistent numbers of samples: [0, 1183]"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "import sklearn\n",
    "cm = sklearn.metrics.confusion_matrix(final_label,final_output)\n",
    "sklearn.metrics.ConfusionMatrixDisplay(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 559,
   "id": "5a8dd00e",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 396/396 [01:32<00:00,  4.28it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from tqdm import tqdm\n",
    "from torchvision import transforms\n",
    "test_data = CustomDataset('test',data_dir,test_label,test_transform)\n",
    "test_dataloader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "test_transform = transforms.Compose([\n",
    "    transforms.ToPILImage(),\n",
    "    transforms.Resize((224, 224)), \n",
    "    transforms.ToTensor(),\n",
    "])\n",
    "#valid_model = torch.load('model.pt')\n",
    "test_model = infer_model\n",
    "test_model.eval()\n",
    "model_preds = []\n",
    "with torch.no_grad():\n",
    "    for i,image in enumerate(tqdm(test_dataloader)):\n",
    "        image = image.to(device)\n",
    "        model_pred = test_model(image)\n",
    "        model_preds += model_pred.argmax(1).detach().cpu().numpy().tolist()\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 560,
   "id": "0cdd2c23",
   "metadata": {},
   "outputs": [],
   "source": [
    "submit = pd.read_csv(f'{data_dir}/sample_submission.csv')\n",
    "submit['artist'] = model_preds\n",
    "submit['artist'] = [label_dict_decode[i] for i in submit['artist']]\n",
    "submit.to_csv('submission.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 561,
   "id": "1a8eb91b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[24, 27, 24, 26, 27, 6, 10, 27, 6, 25, 26, 6, 6, 27, 26, 34, 24, 26, 6, 6, 6, 6, 10, 13, 27, 5, 1, 27, 6, 21, 27, 21, 35, 29, 1, 27, 27, 19, 35, 21, 6, 26, 21, 21, 6, 6, 6, 6, 27, 3, 41, 10, 13, 10, 26, 27, 42, 26, 6, 26, 6, 27, 27, 26, 6, 21, 6, 6, 27, 6, 27, 26, 24, 27, 6, 10, 27, 27, 26, 21, 29, 6, 27, 10, 21, 6, 6, 6, 21, 27, 6, 10, 6, 6, 16, 16, 26, 10, 21, 10, 27, 6, 6, 26, 24, 6, 10, 26, 10, 26, 6, 21, 6, 35, 6, 6, 6, 6, 27, 41, 19, 27, 27, 32, 6, 6, 26, 10, 24, 26, 41, 6, 29, 26, 24, 26, 6, 10, 21, 26, 10, 21, 26, 26, 10, 3, 35, 6, 6, 10, 10, 10, 3, 26, 6, 6, 26, 6, 10, 6, 26, 26, 10, 26, 27, 26, 26, 21, 27, 27, 1, 26, 21, 27, 6, 10, 16, 26, 26, 21, 6, 6, 2, 21, 26, 27, 27, 26, 27, 3, 16, 27, 35, 26, 26, 26, 27, 27, 15, 27, 6, 26, 29, 13, 10, 6, 10, 16, 6, 10, 26, 21, 6, 26, 10, 26, 6, 6, 21, 3, 26, 6, 6, 21, 16, 10, 10, 10, 26, 26, 6, 16, 27, 27, 5, 27, 21, 6, 27, 35, 6, 27, 29, 10, 21, 27, 3, 6, 27, 21, 25, 21, 6, 34, 6, 16, 6, 6, 10, 6, 24, 10, 21, 6, 10, 31, 10, 10, 6, 3, 26, 26, 27, 6, 19, 10, 26, 26, 21, 27, 26, 44, 21, 6, 27, 6, 35, 6, 10, 6, 35, 26, 6, 27, 13, 21, 6, 6, 27, 26, 26, 25, 27, 6, 21, 6, 10, 27, 6, 8, 26, 26, 19, 19, 21, 10, 26, 10, 6, 6, 26, 26, 10, 12, 10, 1, 15, 15, 6, 27, 27, 6, 6, 27, 6, 24, 21, 10, 16, 26, 26, 27, 6, 10, 27, 27, 13, 26, 26, 6, 34, 21, 6, 26, 21, 24, 26, 27, 6, 24, 34, 27, 29, 26, 21, 6, 10, 24, 26, 24, 10, 10, 19, 6, 26, 26, 12, 27, 21, 19, 27, 21, 21, 24, 31, 6, 10, 24, 6, 27, 10, 6, 27, 10, 27, 10, 27, 26, 10, 10, 6, 10, 26, 6, 10, 6, 26, 27, 6, 21, 6, 27, 27, 10, 27, 21, 26, 26, 10, 10, 29, 10, 1, 16, 26, 6, 24, 6, 21, 21, 27, 6, 26, 26, 6, 27, 6, 6, 26, 33, 3, 21, 6, 21, 1, 26, 27, 27, 26, 27, 13, 27, 6, 26, 27, 21, 6, 6, 24, 26, 6, 21, 24, 27, 27, 26, 33, 10, 27, 35, 26, 6, 24, 27, 26, 6, 16, 3, 6, 26, 6, 10, 6, 34, 5, 29, 21, 27, 44, 21, 27, 15, 6, 21, 10, 23, 6, 27, 10, 21, 6, 24, 44, 6, 5, 21, 32, 21, 29, 14, 26, 26, 6, 13, 27, 6, 21, 21, 3, 26, 10, 3, 26, 6, 6, 18, 31, 10, 26, 24, 10, 5, 26, 1, 6, 35, 6, 10, 6, 1, 10, 27, 6, 24, 27, 26, 6, 24, 26, 9, 27, 6, 6, 21, 6, 26, 21, 27, 23, 21, 6, 24, 24, 6, 10, 27, 29, 6, 6, 26, 10, 6, 27, 21, 27, 26, 26, 26, 6, 26, 26, 6, 16, 24, 10, 6, 31, 26, 26, 6, 21, 6, 10, 6, 26, 6, 21, 6, 24, 10, 33, 6, 6, 6, 26, 18, 6, 26, 18, 27, 26, 29, 27, 6, 24, 10, 10, 24, 24, 27, 26, 10, 21, 27, 10, 27, 6, 24, 27, 27, 27, 27, 6, 6, 21, 27, 26, 26, 27, 10, 10, 21, 26, 2, 10, 6, 6, 27, 24, 26, 27, 26, 6, 6, 6, 10, 27, 21, 10, 10, 26, 10, 6, 12, 21, 27, 21, 48, 35, 6, 27, 6, 27, 26, 10, 6, 26, 21, 6, 15, 31, 5, 6, 27, 6, 26, 15, 27, 2, 27, 26, 26, 10, 6, 27, 29, 27, 29, 10, 34, 27, 26, 10, 27, 10, 21, 21, 26, 6, 27, 21, 26, 26, 10, 26, 6, 24, 10, 26, 6, 10, 21, 19, 6, 21, 12, 6, 27, 6, 26, 6, 27, 26, 26, 16, 27, 26, 29, 6, 6, 5, 26, 24, 27, 3, 6, 6, 26, 3, 1, 21, 27, 6, 6, 10, 10, 44, 27, 6, 6, 21, 33, 24, 3, 21, 6, 34, 6, 21, 27, 24, 33, 6, 27, 21, 6, 24, 26, 27, 21, 26, 26, 6, 26, 26, 26, 6, 27, 26, 27, 6, 27, 6, 10, 6, 27, 26, 26, 16, 29, 27, 24, 6, 21, 21, 27, 2, 21, 26, 6, 24, 26, 18, 26, 26, 32, 14, 26, 31, 27, 27, 10, 21, 26, 27, 26, 6, 10, 26, 24, 10, 10, 6, 24, 24, 6, 27, 6, 24, 21, 27, 26, 27, 24, 24, 6, 26, 26, 26, 26, 10, 10, 35, 21, 24, 6, 27, 26, 26, 27, 10, 10, 26, 27, 42, 6, 27, 21, 6, 29, 21, 25, 3, 27, 10, 26, 27, 6, 41, 10, 26, 21, 6, 26, 6, 26, 27, 35, 35, 21, 24, 35, 27, 21, 6, 6, 25, 10, 5, 21, 6, 21, 27, 1, 26, 26, 21, 18, 6, 10, 6, 27, 27, 27, 27, 6, 24, 6, 21, 26, 27, 26, 6, 21, 10, 10, 21, 6, 26, 35, 1, 27, 13, 16, 21, 44, 26, 24, 21, 26, 27, 27, 27, 24, 13, 27, 6, 10, 27, 21, 6, 18, 6, 21, 27, 26, 6, 6, 6, 27, 27, 26, 27, 27, 31, 29, 24, 6, 27, 21, 26, 21, 6, 21, 24, 6, 16, 21, 10, 10, 26, 26, 16, 10, 10, 24, 26, 21, 1, 16, 21, 6, 21, 27, 6, 27, 10, 27, 37, 24, 27, 6, 16, 21, 26, 10, 44, 6, 6, 27, 10, 26, 24, 26, 21, 44, 24, 27, 27, 10, 26, 10, 33, 26, 26, 10, 26, 35, 6, 3, 6, 6, 6, 6, 3, 24, 27, 27, 10, 6, 35, 6, 26, 6, 6, 21, 6, 16, 26, 10, 6, 27, 27, 16, 43, 26, 33, 26, 26, 26, 26, 41, 26, 21, 27, 33, 6, 6, 6, 26, 27, 21, 27, 6, 6, 35, 6, 6, 15, 6, 27, 6, 1, 10, 25, 10, 10, 27, 24, 6, 10, 26, 33, 31, 6, 24, 26, 26, 6, 6, 35, 21, 31, 21, 10, 34, 10, 33, 1, 6, 10, 3, 27, 27, 21, 6, 3, 27, 29, 27, 6, 6, 6, 5, 6, 6, 26, 26, 21, 16, 10, 27, 27, 27, 12, 21, 24, 26, 21, 19, 24, 6, 26, 26, 26, 3, 27, 27, 6, 6, 27, 27, 27, 27, 26, 27, 26, 27, 27, 6, 31, 31, 33, 27, 26, 6, 26, 21, 18, 6, 6, 21, 6, 6, 31, 26, 27, 15, 29, 23, 21, 15, 6, 27, 26, 34, 27, 21, 6, 6, 10, 6, 31, 12, 26, 6, 26, 44, 10, 27, 26, 21, 6, 26, 26, 21, 26, 6, 10, 26, 24, 6, 21, 6, 26, 24, 24, 34, 6, 9, 6, 10, 24, 26, 26, 6, 27, 27, 26, 35, 6, 24, 21, 44, 10, 6, 28, 25, 6, 6, 24, 6, 6, 6, 27, 26, 27, 27, 27, 6, 26, 6, 16, 6, 16, 16, 27, 10, 10, 21, 6, 10, 26, 6, 27, 10, 24, 26, 6, 26, 26, 6, 10, 6, 21, 26, 27, 6, 33, 27, 27, 6, 24, 6, 21, 26, 10, 6, 26, 6, 31, 6, 27, 26, 35, 21, 6, 8, 12, 19, 26, 27, 15, 10, 26, 26, 24, 3, 6, 26, 26, 21, 10, 27, 26, 6, 27, 27, 27, 6, 24, 35, 6, 27, 26, 6, 25, 3, 26, 44, 6, 6, 27, 26, 26, 21, 26, 31, 6, 26, 27, 6, 21, 3, 21, 26, 26, 6, 28, 24, 6, 27, 27, 21, 26, 6, 27, 21, 21, 26, 21, 21, 21, 10, 6, 6, 6, 21, 24, 26, 6, 26, 29, 26, 26, 6, 10, 41, 10, 10, 29, 24, 27, 6, 6, 10, 6, 1, 26, 35, 10, 21, 21, 27, 6, 26, 29, 27, 1, 16, 21, 27, 27, 21, 10, 6, 27, 6, 10, 27, 6, 6, 27, 27, 6, 6, 10, 24, 24, 24, 27, 21, 6, 6, 18, 10, 21, 6, 27, 6, 26, 26, 10, 26, 24, 27, 4, 21, 27, 27, 21, 6, 29, 26, 21, 10, 1, 26, 10, 21, 10, 6, 31, 6, 6, 6, 16, 6, 26, 21, 24, 26, 10, 10, 10, 24, 6, 26, 27, 6, 26, 27, 10, 10, 6, 26, 24, 21, 27, 6, 10, 27, 27, 10, 29, 26, 6, 26, 27, 21, 6, 19, 6, 6, 27, 6, 21, 1, 10, 26, 6, 27, 16, 27, 21, 41, 16, 21, 10, 21, 27, 27, 27, 6, 26, 6, 6, 27, 21, 27, 6, 21, 27, 21, 6, 27, 6, 26, 27, 6, 16, 27, 10, 6, 21, 27, 21, 21, 26, 10, 6, 19, 27, 10, 26, 10, 29, 26, 27, 3, 26, 26, 10, 27, 27, 26, 10, 6, 26, 21, 6, 6, 10, 6, 27, 26, 6, 1, 26, 26, 1, 10, 6, 6, 27, 1, 29, 26, 21, 6, 6, 24, 27, 21, 34, 27, 24, 26, 24, 10, 10, 27, 29, 24, 26, 6, 10, 26, 16, 29, 26, 6, 26, 27, 34, 1, 27, 6, 26, 26, 10, 6, 27, 34, 24, 6, 31, 24, 15, 26, 6, 16, 27, 21, 6, 21, 26, 6, 24, 13, 6, 27, 26, 6, 26, 27, 26, 15, 26, 12, 33, 6, 6, 10, 26, 6, 21, 6, 41, 6, 21, 14, 27, 10, 6, 24, 27, 10, 26, 10, 6, 27, 44, 21, 21, 27, 19, 27, 26, 6, 26, 3, 10, 35, 6, 33, 27, 26, 14, 24, 21, 32, 6, 10, 27, 27, 34, 10, 10, 10, 28, 33, 10, 27, 26, 10, 6, 10, 29, 6, 1, 21, 10, 10, 26, 1, 10, 29, 6, 10, 6, 6, 27, 6, 26, 27, 27, 27, 6, 26, 24, 26, 21, 6, 44, 26, 21, 26, 27, 26, 27, 26, 6, 26, 6, 21, 10, 21, 27, 13, 27, 10, 24, 6, 27, 6, 6, 27, 24, 6, 18, 26, 26, 6, 6, 27, 27, 24, 6, 26, 26, 27, 6, 24, 27, 16, 41, 27, 27, 26, 26, 26, 41, 6, 26, 6, 15, 34, 21, 10, 6, 26, 26, 27, 26, 27, 3, 6, 27, 14, 18, 27, 6, 35, 26, 6, 27, 27, 27, 6, 44, 26, 29, 31, 35, 26, 34, 27, 10, 10, 26, 21, 21, 6, 27, 6, 26, 26, 27, 6, 10, 27, 26, 26, 26, 10, 27, 26, 27, 10, 6, 44, 6, 6, 6, 27, 6, 6, 31, 26, 26, 26, 27, 25, 27, 27, 27, 21, 10, 27, 6, 6, 26, 3, 26, 27, 16, 1, 6, 26, 6, 26, 21, 16, 6, 21, 10, 26, 27, 27, 44, 6, 3, 21, 10, 26, 27, 27, 27, 21, 6, 27, 21, 16, 27, 26, 6, 26, 26, 26, 26, 27, 24, 10, 24, 6, 21, 10, 21, 27, 6, 34, 27, 18, 27, 6, 33, 16, 3, 6, 6, 6, 26, 6, 6, 6, 19, 21, 33, 26, 31, 29, 6, 27, 10, 26, 26, 1, 21, 26, 10, 18, 21, 26, 12, 26, 26, 10, 24, 21, 9, 27, 6, 28, 16, 35, 24, 27, 10, 26, 6, 26, 6, 6, 10, 6, 5, 16, 28, 12, 26, 6, 6, 44, 6, 26, 27, 6, 26, 26, 26, 6, 24, 27, 10, 26, 24, 6, 6, 27, 6, 29, 6, 6, 31, 26, 21, 23, 21, 6, 6, 26, 27, 33, 6, 26, 27, 24, 6, 1, 27, 2, 10, 27, 27, 15, 26, 26, 26, 6, 13, 10, 27, 24, 6, 10, 10, 25, 26, 26, 27, 21, 6, 10, 21, 6, 19, 16, 6, 6, 6, 6, 21, 27, 27, 10, 10, 27, 6, 6, 26, 27, 10, 10, 10, 24, 29, 21, 24, 13, 27, 10, 27, 27, 26, 18, 2, 1, 26, 27, 26, 26, 6, 24, 10, 3, 24, 21, 26, 6, 27, 41, 10, 26, 26, 21, 10, 6, 26, 19, 6, 35, 26, 24, 6, 27, 6, 10, 21, 6, 26, 6, 6, 1, 27, 21, 27, 31, 6, 27, 10, 6, 26, 21, 21, 24, 24, 26, 26, 31, 6, 26, 27, 6, 26, 27, 6, 15, 27, 10, 15, 6, 3, 33, 21, 21, 1, 6, 26, 21, 27, 27, 27, 6, 31, 34, 26, 42, 21, 6, 27, 6, 27, 18, 6, 1, 24, 26, 10, 6, 10, 27, 41, 27, 10, 21, 6, 21, 6, 27, 6, 24, 38, 10, 6, 26, 6, 26, 21, 6, 6, 6, 6, 26, 10, 6, 6, 21, 35, 6, 21, 27, 10, 34, 27, 21, 6, 27, 27, 24, 19, 27, 6, 6, 21, 27, 26, 26, 27, 24, 33, 2, 26, 26, 26, 6, 21, 26, 21, 26, 15, 24, 24, 27, 6, 6, 10, 33, 6, 6, 26, 10, 21, 6, 27, 26, 26, 21, 27, 10, 21, 27, 26, 26, 26, 10, 27, 1, 26, 24, 12, 26, 27, 10, 6, 26, 10, 10, 27, 27, 6, 21, 21, 6, 27, 26, 27, 15, 26, 21, 26, 21, 26, 10, 21, 26, 24, 10, 10, 6, 6, 1, 26, 24, 16, 10, 27, 31, 27, 6, 24, 12, 6, 26, 24, 29, 6, 27, 26, 6, 26, 10, 27, 26, 6, 10, 10, 6, 27, 27, 27, 10, 21, 10, 6, 24, 27, 21, 6, 10, 24, 27, 31, 6, 13, 21, 26, 6, 26, 27, 26, 10, 10, 27, 6, 26, 21, 26, 6, 10, 10, 26, 3, 26, 27, 29, 6, 10, 24, 10, 27, 6, 21, 27, 6, 35, 26, 27, 24, 21, 27, 25, 10, 35, 21, 28, 24, 27, 21, 26, 26, 6, 26, 5, 27, 6, 26, 27, 21, 27, 27, 21, 26, 27, 26, 21, 27, 26, 6, 27, 1, 10, 24, 26, 31, 27, 26, 27, 26, 27, 6, 1, 31, 26, 27, 27, 10, 6, 27, 6, 31, 3, 27, 29, 31, 1, 27, 10, 6, 26, 10, 10, 29, 26, 13, 34, 27, 26, 26, 26, 15, 10, 27, 6, 27, 27, 26, 21, 27, 24, 38, 6, 26, 10, 21, 6, 32, 21, 3, 6, 6, 27, 6, 16, 27, 6, 27, 21, 6, 16, 10, 1, 21, 27, 27, 26, 10, 32, 6, 26, 26, 6, 41, 26, 6, 10, 37, 21, 10, 27, 6, 26, 6, 26, 29, 44, 6, 27, 27, 27, 6, 27, 10, 6, 21, 26, 38, 27, 16, 6, 21, 6, 24, 6, 26, 29, 21, 35, 45, 33, 23, 10, 10, 35, 6, 10, 6, 29, 6, 27, 26, 27, 26, 33, 21, 21, 26, 21, 26, 27, 35, 6, 10, 26, 10, 27, 6, 26, 21, 29, 16, 19, 26, 35, 35, 10, 21, 21, 10, 12, 27, 6, 26, 10, 26, 1, 27, 27, 10, 6, 10, 6, 6, 21, 26, 27, 32, 13, 24, 24, 26, 26, 10, 26, 6, 6, 21, 10, 27, 24, 10, 26, 6, 27, 27, 31, 27, 35, 6, 10, 21, 26, 26, 24, 27, 12, 16, 27, 27, 6, 3, 26, 27, 26, 26, 3, 6, 26, 27, 6, 27, 24, 23, 27, 10, 27, 10, 26, 10, 21, 27, 27, 27, 27, 10, 26, 24, 3, 27, 27, 19, 18, 6, 24, 21, 26, 24, 24, 21, 21, 6, 27, 16, 10, 27, 27, 6, 6, 27, 26, 26, 3, 26, 29, 21, 27, 27, 10, 27, 29, 27, 6, 29, 24, 24, 3, 10, 10, 16, 21, 21, 34, 21, 16, 27, 19, 26, 21, 12, 24, 27, 26, 26, 24, 10, 26, 26, 16, 6, 21, 3, 26, 21, 27, 25, 26, 27, 26, 6, 21, 10, 26, 6, 28, 27, 2, 26, 6, 24, 33, 27, 27, 6, 1, 31, 6, 24, 6, 26, 16, 5, 6, 21, 26, 27, 6, 6, 23, 10, 16, 26, 26, 26, 3, 44, 6, 26, 27, 26, 26, 24, 6, 26, 6, 29, 26, 35, 26, 6, 26, 26, 25, 34, 1, 27, 21, 6, 21, 27, 10, 6, 26, 10, 26, 26, 1, 19, 24, 26, 21, 3, 6, 35, 27, 26, 27, 10, 43, 10, 27, 6, 6, 10, 26, 26, 6, 27, 26, 6, 6, 1, 35, 35, 6, 6, 34, 29, 26, 6, 26, 6, 41, 26, 34, 21, 31, 27, 26, 21, 21, 26, 26, 6, 26, 6, 6, 26, 6, 27, 6, 6, 25, 24, 37, 10, 21, 10, 16, 27, 26, 29, 6, 35, 10, 27, 26, 26, 21, 27, 21, 6, 25, 24, 27, 16, 10, 6, 26, 27, 6, 24, 6, 6, 27, 16, 6, 12, 6, 26, 21, 6, 27, 10, 6, 26, 10, 27, 10, 24, 26, 27, 28, 27, 1, 26, 26, 10, 10, 35, 10, 21, 13, 26, 6, 27, 29, 18, 28, 27, 21, 33, 6, 27, 27, 27, 16, 26, 3, 27, 27, 24, 24, 6, 6, 27, 21, 24, 6, 21, 26, 35, 38, 16, 6, 27, 10, 10, 6, 21, 21, 27, 26, 24, 6, 26, 26, 33, 26, 32, 6, 6, 10, 12, 29, 27, 3, 26, 15, 26, 18, 27, 24, 10, 10, 44, 26, 26, 2, 26, 6, 19, 16, 27, 27, 6, 6, 25, 6, 26, 24, 24, 6, 26, 18, 26, 27, 2, 10, 34, 16, 26, 27, 27, 6, 27, 6, 6, 6, 24, 26, 21, 10, 37, 10, 6, 26, 31, 10, 27, 21, 26, 24, 24, 26, 26, 26, 24, 6, 6, 6, 27, 27, 26, 6, 6, 10, 24, 6, 24, 24, 27, 41, 27, 7, 19, 29, 24, 26, 27, 16, 6, 27, 26, 6, 26, 23, 35, 13, 26, 6, 41, 21, 21, 27, 6, 6, 26, 6, 6, 27, 10, 27, 10, 26, 26, 5, 10, 6, 6, 27, 6, 27, 10, 29, 21, 27, 3, 19, 27, 26, 27, 26, 10, 6, 24, 6, 6, 6, 26, 26, 24, 21, 26, 21, 26, 28, 29, 10, 21, 34, 32, 10, 24, 16, 27, 15, 21, 26, 21, 12, 10, 27, 27, 29, 6, 21, 21, 27, 27, 26, 6, 3, 21, 6, 26, 21, 27, 27, 33, 21, 31, 41, 26, 6, 35, 21, 21, 21, 27, 6, 27, 21, 3, 37, 14, 27, 6, 6, 21, 6, 21, 10, 1, 27, 3, 6, 15, 26, 31, 6, 21, 34, 10, 18, 27, 26, 27, 24, 10, 21, 6, 27, 26, 6, 27, 18, 27, 6, 26, 1, 27, 16, 26, 6, 21, 27, 27, 21, 10, 6, 26, 26, 24, 6, 26, 6, 26, 6, 10, 26, 27, 10, 6, 24, 12, 6, 29, 6, 26, 10, 27, 21, 6, 27, 6, 21, 6, 26, 6, 26, 10, 15, 26, 27, 34, 21, 34, 27, 26, 6, 32, 24, 10, 10, 6, 25, 27, 26, 26, 21, 10, 26, 16, 26, 26, 16, 31, 12, 6, 35, 18, 13, 10, 26, 6, 21, 26, 10, 10, 10, 26, 27, 24, 27, 27, 33, 16, 10, 21, 10, 16, 6, 26, 3, 31, 6, 24, 27, 6, 27, 21, 6, 20, 10, 10, 25, 16, 21, 26, 3, 26, 10, 10, 27, 21, 26, 3, 24, 21, 26, 29, 24, 41, 26, 26, 16, 26, 27, 10, 6, 32, 6, 10, 26, 3, 27, 24, 31, 6, 26, 21, 27, 26, 6, 10, 6, 27, 6, 6, 16, 21, 26, 6, 24, 26, 26, 1, 21, 26, 26, 10, 26, 6, 6, 26, 27, 35, 21, 6, 35, 27, 26, 6, 26, 35, 26, 10, 26, 29, 24, 26, 41, 12, 6, 27, 31, 29, 21, 24, 26, 27, 10, 26, 27, 6, 6, 26, 21, 6, 26, 10, 27, 10, 6, 27, 6, 29, 12, 6, 6, 26, 21, 5, 6, 26, 15, 27, 26, 24, 25, 31, 25, 24, 35, 27, 6, 16, 10, 6, 6, 12, 6, 26, 27, 26, 21, 24, 27, 21, 19, 18, 26, 21, 3, 13, 6, 27, 26, 26, 6, 27, 24, 27, 6, 6, 26, 6, 10, 6, 21, 10, 10, 26, 10, 6, 6, 6, 21, 6, 44, 21, 26, 27, 26, 24, 27, 6, 27, 27, 13, 26, 27, 24, 26, 6, 26, 13, 6, 27, 6, 26, 27, 26, 10, 21, 26, 26, 33, 6, 21, 6, 12, 6, 6, 10, 10, 10, 27, 6, 24, 6, 10, 27, 26, 6, 29, 31, 26, 18, 12, 6, 6, 10, 6, 25, 26, 27, 26, 21, 6, 26, 10, 26, 27, 6, 32, 10, 26, 6, 26, 26, 10, 26, 33, 26, 6, 35, 24, 3, 6, 6, 26, 26, 21, 6, 10, 27, 26, 27, 29, 10, 27, 6, 10, 29, 6, 19, 26, 24, 27, 26, 27, 26, 26, 35, 6, 26, 6, 10, 6, 12, 26, 21, 27, 29, 26, 10, 3, 26, 26, 21, 26, 26, 3, 10, 24, 6, 26, 26, 26, 26, 6, 26, 25, 26, 10, 27, 23, 21, 26, 27, 6, 6, 10, 26, 27, 27, 27, 15, 21, 10, 6, 24, 26, 0, 26, 21, 6, 6, 42, 6, 5, 27, 21, 26, 26, 26, 18, 10, 21, 24, 26, 21, 3, 26, 21, 21, 16, 26, 26, 10, 26, 24, 16, 27, 26, 6, 6, 27, 16, 26, 6, 10, 27, 26, 26, 6, 3, 24, 6, 26, 16, 6, 27, 27, 27, 6, 26, 29, 10, 26, 35, 26, 21, 18, 6, 29, 21, 6, 6, 26, 27, 27, 6, 27, 26, 31, 27, 21, 6, 26, 27, 10, 6, 1, 27, 10, 26, 10, 6, 9, 27, 6, 6, 28, 24, 6, 33, 32, 16, 10, 21, 26, 6, 27, 6, 2, 3, 6, 25, 26, 28, 35, 6, 24, 10, 10, 26, 10, 24, 21, 24, 6, 27, 16, 25, 6, 27, 10, 6, 6, 6, 26, 1, 10, 34, 6, 6, 21, 26, 27, 16, 27, 21, 6, 28, 6, 26, 6, 21, 21, 26, 24, 26, 6, 24, 26, 21, 27, 12, 6, 26, 1, 19, 10, 21, 10, 26, 21, 27, 10, 6, 6, 27, 27, 21, 24, 10, 27, 10, 26, 26, 26, 3, 6, 27, 1, 10, 26, 26, 6, 27, 35, 24, 23, 27, 29, 12, 24, 6, 27, 5, 26, 6, 29, 28, 3, 26, 31, 10, 13, 26, 10, 6, 27, 26, 27, 35, 6, 6, 26, 24, 6, 6, 15, 6, 26, 37, 10, 10, 27, 21, 26, 10, 21, 6, 21, 10, 10, 6, 6, 24, 6, 10, 6, 26, 10, 6, 27, 6, 6, 6, 26, 26, 6, 24, 15, 10, 10, 10, 27, 21, 6, 27, 21, 6, 27, 6, 26, 14, 6, 26, 21, 24, 21, 13, 6, 24, 6, 6, 26, 6, 26, 26, 27, 21, 27, 27, 1, 27, 29, 26, 6, 26, 26, 10, 27, 6, 26, 29, 27, 26, 26, 21, 26, 26, 21, 6, 6, 21, 27, 27, 16, 10, 27, 27, 29, 12, 6, 27, 27, 26, 3, 6, 27, 35, 6, 3, 29, 27, 6, 21, 10, 26, 16, 26, 27, 21, 24, 26, 27, 27, 1, 10, 10, 6, 27, 10, 6, 27, 41, 27, 24, 10, 26, 21, 26, 27, 6, 6, 10, 21, 26, 10, 6, 6, 27, 3, 26, 29, 10, 26, 6, 10, 26, 27, 21, 10, 6, 27, 26, 32, 6, 29, 24, 6, 26, 10, 33, 24, 6, 6, 21, 24, 26, 26, 6, 6, 6, 10, 26, 27, 15, 27, 24, 26, 21, 26, 27, 24, 32, 6, 26, 21, 21, 26, 27, 33, 10, 21, 10, 10, 3, 27, 10, 26, 27, 10, 24, 18, 26, 13, 26, 6, 6, 21, 27, 6, 21, 31, 27, 21, 24, 27, 26, 27, 26, 26, 26, 26, 6, 27, 26, 10, 12, 27, 6, 10, 6, 6, 25, 10, 21, 21, 26, 21, 6, 10, 6, 6, 26, 26, 6, 33, 33, 23, 27, 26, 26, 10, 21, 26, 24, 19, 6, 6, 21, 23, 6, 26, 24, 33, 26, 21, 14, 27, 10, 6, 6, 26, 26, 15, 16, 26, 6, 10, 27, 10, 5, 26, 27, 44, 1, 6, 26, 27, 6, 3, 10, 23, 27, 10, 19, 10, 10, 6, 27, 26, 26, 27, 29, 10, 21, 27, 6, 27, 26, 10, 16, 0, 27, 24, 24, 35, 18, 26, 21, 38, 6, 26, 21, 12, 21, 24, 1, 26, 27, 24, 27, 6, 27, 6, 27, 26, 6, 12, 6, 6, 34, 26, 18, 21, 10, 6, 26, 10, 6, 31, 24, 6, 6, 27, 6, 26, 6, 27, 10, 21, 10, 27, 27, 10, 6, 27, 26, 3, 6, 10, 6, 27, 26, 21, 3, 26, 19, 6, 26, 24, 27, 21, 10, 27, 6, 26, 26, 26, 3, 12, 6, 10, 10, 10, 26, 26, 6, 26, 26, 13, 26, 35, 28, 6, 6, 35, 10, 6, 27, 35, 6, 27, 10, 6, 6, 26, 6, 10, 26, 26, 26, 27, 6, 26, 27, 27, 10, 29, 10, 27, 27, 6, 27, 24, 33, 6, 16, 6, 21, 15, 23, 27, 26, 24, 24, 26, 6, 27, 27, 6, 26, 6, 21, 26, 29, 10, 27, 26, 6, 21, 19, 26, 27, 21, 27, 24, 26, 3, 10, 13, 27, 31, 6, 10, 6, 6, 25, 6, 6, 10, 31, 16, 34, 21, 27, 24, 21, 26, 26, 6, 1, 27, 27, 6, 6, 26, 27, 27, 33, 29, 6, 27, 26, 1, 27, 29, 27, 6, 26, 6, 3, 24, 26, 26, 27, 21, 26, 6, 31, 21, 26, 25, 5, 10, 21, 10, 24, 6, 25, 10, 21, 12, 6, 16, 27, 6, 27, 21, 10, 10, 33, 27, 6, 26, 31, 26, 34, 10, 10, 27, 21, 10, 10, 6, 6, 27, 10, 10, 10, 6, 6, 27, 26, 27, 24, 6, 6, 41, 6, 26, 44, 27, 26, 21, 26, 15, 6, 32, 26, 6, 10, 21, 6, 27, 10, 26, 26, 10, 10, 27, 27, 6, 26, 1, 26, 1, 26, 21, 6, 27, 24, 15, 10, 27, 21, 6, 24, 26, 27, 15, 29, 27, 27, 15, 26, 21, 24, 6, 1, 27, 23, 21, 6, 21, 21, 27, 27, 26, 16, 21, 37, 29, 10, 35, 6, 6, 26, 10, 23, 29, 21, 24, 21, 6, 27, 24, 26, 27, 26, 21, 27, 26, 24, 35, 26, 24, 10, 6, 27, 27, 31, 6, 10, 27, 24, 10, 21, 16, 16, 6, 21, 27, 6, 6, 27, 21, 44, 6, 14, 15, 10, 15, 6, 6, 44, 6, 26, 19, 1, 21, 21, 33, 6, 10, 27, 33, 16, 21, 27, 26, 26, 6, 10, 26, 6, 19, 21, 26, 29, 23, 6, 26, 6, 10, 10, 9, 6, 26, 6, 27, 27, 26, 27, 10, 27, 6, 28, 10, 26, 6, 24, 27, 6, 6, 6, 26, 6, 1, 24, 21, 35, 13, 21, 26, 21, 29, 21, 27, 27, 6, 41, 21, 10, 6, 21, 26, 13, 27, 27, 1, 26, 16, 6, 16, 27, 34, 13, 26, 27, 6, 35, 26, 29, 27, 6, 10, 26, 10, 26, 3, 10, 6, 24, 1, 42, 6, 19, 26, 16, 21, 26, 6, 27, 10, 6, 26, 27, 27, 6, 27, 10, 10, 27, 31, 26, 26, 27, 10, 6, 10, 6, 27, 10, 31, 6, 6, 26, 26, 26, 26, 37, 27, 6, 27, 24, 6, 6, 15, 6, 27, 6, 27, 27, 10, 6, 16, 27, 27, 6, 6, 10, 24, 27, 21, 27, 6, 24, 10, 27, 1, 10, 21, 6, 26, 26, 6, 32, 29, 29, 16, 6, 24, 26, 18, 21, 10, 10, 27, 27, 9, 26, 6, 24, 27, 27, 6, 15, 27, 27, 6, 27, 31, 27, 6, 29, 6, 26, 11, 26, 27, 24, 27, 21, 6, 15, 10, 10, 24, 27, 24, 6, 24, 6, 26, 24, 26, 26, 6, 21, 27, 10, 26, 6, 24, 26, 26, 26, 21, 6, 26, 21, 19, 6, 6, 26, 6, 21, 6, 41, 26, 6, 6, 27, 6, 24, 26, 13, 10, 10, 10, 26, 16, 6, 6, 10, 6, 10, 27, 6, 29, 10, 24, 21, 6, 6, 10, 26, 6, 21, 27, 6, 27, 26, 6, 21, 6, 26, 21, 26, 26, 5, 21, 27, 21, 16, 6, 6, 21, 6, 10, 44, 6, 21, 24, 26, 24, 10, 6, 26, 10, 27, 21, 6, 26, 26, 3, 26, 6, 27, 1, 26, 10, 21, 10, 12, 3, 26, 27, 24, 27, 26, 26, 27, 6, 6, 3, 6, 10, 6, 21, 31, 26, 21, 21, 26, 26, 26, 10, 26, 26, 27, 19, 27, 35, 27, 10, 18, 21, 26, 21, 26, 26, 27, 10, 27, 26, 27, 21, 33, 10, 21, 31, 6, 41, 26, 16, 6, 24, 10, 35, 10, 21, 29, 10, 27, 6, 21, 21, 26, 24, 27, 44, 24, 26, 27, 10, 26, 27, 6, 24, 27, 24, 18, 24, 10, 21, 27, 16, 10, 27, 35, 6, 25, 26, 6, 6, 21, 26, 27, 27, 26, 27, 26, 41, 26, 21, 10, 1, 27, 10, 26, 10, 26, 10, 27, 21, 6, 26, 26, 27, 21, 6, 6, 6, 23, 26, 6, 24, 35, 6, 6, 35, 6, 27, 21, 26, 26, 21, 27, 24, 6, 6, 15, 26, 6, 6, 10, 26, 6, 6, 26, 10, 21, 24, 26, 26, 6, 27, 24, 27, 10, 6, 21, 26, 13, 6, 26, 27, 26, 26, 26, 27, 27, 21, 1, 10, 21, 26, 6, 27, 27, 27, 21, 1, 44, 6, 6, 28, 6, 27, 15, 26, 44, 21, 6, 27, 27, 27, 10, 44, 27, 24, 10, 27, 26, 27, 6, 6, 26, 10, 6, 6, 10, 10, 27, 26, 29, 1, 26, 6, 10, 26, 10, 6, 3, 10, 29, 21, 27, 24, 16, 9, 26, 21, 25, 29, 10, 16, 26, 6, 27, 19, 10, 26, 26, 21, 9, 26, 27, 6, 35, 21, 10, 6, 16, 31, 26, 26, 6, 21, 27, 6, 26, 10, 31, 21, 6, 10, 26, 27, 41, 31, 21, 44, 21, 21, 23, 6, 27, 10, 6, 26, 6, 34, 6, 10, 19, 27, 27, 6, 27, 16, 8, 6, 10, 6, 16, 21, 6, 3, 10, 6, 26, 26, 27, 19, 6, 10, 34, 21, 26, 27, 26, 44, 21, 24, 6, 10, 10, 10, 26, 3, 26, 6, 6, 26, 25, 26, 26, 21, 24, 6, 6, 6, 10, 6, 10, 24, 6, 21, 10, 6, 6, 24, 27, 26, 21, 27, 6, 21, 42, 31, 10, 6, 6, 19, 37, 24, 27, 21, 6, 21, 28, 6, 21, 10, 6, 27, 33, 6, 21, 27, 24, 6, 26, 24, 24, 21, 10, 21, 21, 26, 27, 27, 27, 26, 24, 6, 27, 10, 6, 26, 12, 6, 26, 26, 6, 21, 26, 32, 24, 27, 6, 1, 3, 6, 27, 19, 26, 26, 24, 27, 6, 24, 25, 1, 26, 27, 3, 6, 33, 10, 16, 26, 6, 44, 21, 24, 27, 26, 10, 24, 27, 29, 26, 44, 27, 26, 10, 35, 1, 27, 6, 27, 21, 6, 1, 6, 44, 10, 26, 29, 15, 26, 6, 6, 29, 25, 15, 6, 10, 6, 27, 6, 19, 27, 10, 6, 24, 10, 6, 24, 26, 6, 10, 27, 6, 21, 31, 27, 24, 26, 6, 29, 26, 26, 26, 21, 24, 27, 29, 8, 26, 26, 26, 24, 16, 6, 24, 10, 6, 35, 27, 26, 26, 12, 26, 24, 6, 6, 26, 25, 6, 27, 21, 6, 6, 26, 21, 6, 27, 15, 10, 6, 26, 6, 21, 6, 27, 6, 26, 27, 26, 6, 27, 6, 21, 6, 6, 6, 27, 26, 21, 10, 15, 24, 21, 6, 26, 10, 27, 14, 6, 26, 6, 27, 27, 6, 26, 10, 26, 31, 41, 6, 10, 6, 12, 31, 6, 26, 6, 26, 6, 26, 10, 26, 26, 6, 19, 11, 6, 16, 28, 26, 21, 13, 24, 21, 24, 27, 26, 15, 6, 32, 6, 27, 10, 31, 27, 6, 27, 21, 26, 21, 27, 16, 6, 27, 3, 10, 24, 21, 10, 1, 6, 19, 27, 5, 44, 16, 6, 6, 6, 24, 26, 6, 10, 33, 27, 27, 27, 27, 6, 10, 27, 41, 10, 10, 6, 26, 35, 10, 26, 27, 10, 27, 21, 29, 27, 26, 27, 29, 26, 27, 27, 27, 6, 6, 16, 5, 24, 6, 29, 29, 24, 21, 10, 6, 24, 6, 3, 6, 6, 16, 27, 27, 26, 24, 29, 27, 21, 32, 21, 6, 26, 10, 6, 21, 26, 6, 27, 1, 27, 10, 27, 27, 12, 27, 10, 21, 24, 10, 10, 24, 26, 6, 27, 27, 27, 24, 10, 16, 33, 27, 29, 26, 33, 13, 35, 24, 27, 26, 10, 10, 27, 35, 44, 29, 24, 27, 6, 6, 6, 6, 24, 10, 26, 10, 10, 26, 27, 16, 6, 26, 10, 6, 26, 6, 27, 24, 21, 24, 26, 29, 27, 10, 18, 6, 6, 26, 42, 6, 6, 41, 6, 26, 21, 6, 27, 27, 6, 27, 10, 6, 15, 6, 27, 28, 31, 10, 27, 31, 24, 21, 27, 26, 6, 12, 26, 6, 6, 6, 26, 6, 26, 27, 31, 26, 26, 26, 21, 6, 6, 27, 10, 26, 26, 10, 24, 6, 27, 24, 5, 1, 1, 21, 10, 27, 31, 27, 10, 2, 21, 1, 6, 15, 21, 10, 21, 27, 6, 21, 24, 1, 6, 21, 6, 21, 6, 1, 15, 27, 27, 10, 21, 6, 21, 21, 6, 27, 10, 6, 10, 27, 6, 6, 21, 6, 26, 21, 27, 24, 1, 6, 10, 27, 27, 6, 6, 26, 16, 10, 6, 26, 26, 6, 21, 27, 13, 27, 6, 26, 6, 16, 6, 10, 26, 26, 37, 6, 3, 10, 27, 6, 3, 34, 27, 6, 6, 27, 10, 27, 27, 27, 21, 26, 10, 10, 6, 6, 26, 27, 21, 21, 6, 10, 21, 10, 27, 25, 27, 6, 10, 24, 6, 24, 21, 26, 26, 6, 27, 21, 24, 27, 6, 24, 24, 41, 26, 21, 10, 26, 26, 3, 6, 10, 21, 6, 21, 31, 27, 10, 27, 1, 26, 21, 10, 26, 21, 27, 27, 6, 21, 26, 24, 27, 26, 6, 27, 26, 26, 21, 6, 18, 10, 19, 21, 10, 10, 24, 10, 42, 27, 6, 26, 3, 27, 21, 34, 26, 32, 13, 26, 27, 6, 10, 6, 2, 29, 21, 2, 6, 6, 3, 24, 10, 27, 26, 27, 27, 26, 10, 26, 6, 16, 6, 27, 26, 6, 6, 24, 26, 26, 19, 6, 44, 26, 21, 6, 6, 6, 25, 26, 34, 6, 27, 26, 21, 10, 27, 27, 27, 1, 26, 6, 6, 6, 10, 27, 26, 10, 21, 24, 26, 27, 26, 26, 10, 21, 10, 6, 15, 1, 26, 6, 10, 6, 27, 6, 6, 29, 26, 26, 26, 15, 3, 27, 6, 10, 6, 10, 6, 6, 10, 29, 31, 21, 25, 6, 3, 27, 6, 24, 6, 24, 24, 6, 24, 6, 27, 26, 26, 27, 10, 16, 6, 6, 6, 26, 19, 27, 13, 6, 11, 6, 27, 10, 6, 21, 24, 27, 6, 21, 10, 27, 26, 27, 21, 27, 10, 21, 16, 10, 21, 21, 21, 21, 26, 26, 10, 27, 10, 10, 6, 6, 24, 6, 27, 10, 6, 27, 1, 27, 29, 26, 3, 31, 24, 21, 10, 10, 3, 6, 33, 6, 26, 6, 27, 27, 26, 21, 10, 10, 10, 24, 21, 10, 6, 6, 6, 26, 21, 26, 21, 32, 27, 10, 26, 21, 6, 23, 6, 26, 21, 21, 26, 44, 13, 24, 6, 27, 2, 31, 6, 6, 24, 26, 26, 27, 26, 24, 6, 3, 6, 10, 15, 19, 6, 6, 17, 10, 26, 6, 3, 27, 10, 10, 26, 35, 21, 26, 27, 6, 27, 10, 3, 33, 27, 6, 27, 10, 24, 21, 34, 27, 27, 29, 6, 21, 10, 10, 10, 6, 24, 27, 26, 27, 6, 6, 27, 31, 9, 21, 6, 26, 26, 29, 13, 27, 26, 6, 31, 6, 21, 26, 27, 10, 10, 27, 35, 23, 10, 6, 21, 29, 26, 26, 27, 27, 21, 27, 33, 1, 10, 6, 6, 26, 26, 6, 10, 26, 27, 35, 6, 26, 21, 6, 27, 35, 10, 27, 10, 6, 21, 21, 10, 10, 26, 6, 13, 10, 10, 6, 26, 29, 6, 27, 21, 10, 16, 26, 27, 24, 19, 27, 6, 27, 10, 24, 6, 24, 10, 6, 10, 26, 6, 27, 16, 6, 6, 26, 6, 21, 10, 3, 26, 10, 27, 21, 10, 1, 24, 26, 24, 21, 10, 26, 44, 27, 10, 10, 12, 6, 10, 6, 27, 29, 27, 33, 21, 10, 6, 33, 27, 24, 25, 21, 10, 6, 26, 6, 6, 10, 24, 21, 14, 10, 6, 16, 6, 24, 10, 24, 26, 6, 15, 27, 27, 21, 10, 6, 34, 6, 10, 26, 26, 26, 25, 27, 26, 10, 21, 10, 6, 27, 6, 21, 27, 6, 27, 6, 24, 6, 27, 26, 6, 24, 10, 44, 10, 6, 12, 27, 6, 6, 6, 10, 16, 33, 21, 6, 27, 10, 10, 10, 27, 12, 6, 26, 16, 21, 10, 24, 21, 27, 27, 16, 26, 26, 26, 6, 6, 10, 6, 21, 6, 26, 6, 26, 16, 24, 25, 26, 26, 26, 21, 24, 24, 6, 6, 27, 6, 27, 6, 6, 27, 21, 16, 6, 10, 26, 6, 10, 10, 21, 6, 27, 6, 10, 21, 26, 10, 6, 26, 10, 29, 6, 24, 6, 6, 6, 19, 26, 26, 10, 24, 10, 21, 26, 27, 26, 21, 27, 21, 21, 6, 6, 21, 6, 27, 26, 27, 19, 10, 44, 21, 6, 29, 6, 6, 27, 27, 27, 5, 10, 6, 3, 10, 26, 35, 27, 21, 35, 35, 21, 6, 26, 27, 21, 19, 24, 6, 10, 24, 6, 10, 21, 35, 27, 10, 10, 6, 6, 26, 10, 10, 6, 21, 6, 6, 21, 6, 27, 21, 24, 10, 26, 6, 27, 10, 24, 27, 28, 21, 26, 26, 15, 26, 44, 21, 26, 10, 35, 26, 21, 35, 26, 21, 27, 21, 26, 13, 10, 21, 6, 26, 10, 41, 15, 27, 6, 35, 6, 6, 21, 10, 26, 26, 6, 26, 21, 31, 27, 10, 21, 6, 26, 6, 6, 27, 26, 26, 10, 10, 6, 26, 26, 26, 27, 26, 26, 26, 21, 27, 3, 21, 41, 26, 27, 27, 26, 27, 6, 10, 27, 6, 41, 10, 3, 24, 10, 26, 6, 10, 26, 10, 6, 10, 10, 28, 10, 27, 24, 6, 41, 10, 24, 10, 27, 26, 6, 21, 26, 6, 6, 19, 27, 24, 24, 16, 6, 10, 6, 16, 26, 6, 6, 27, 27, 6, 6, 10, 27, 21, 26, 27, 15, 29, 6, 27, 10, 27, 19, 21, 21, 24, 6, 34, 6, 6, 37, 16, 10, 26, 27, 24, 21, 19, 24, 21, 44, 26, 34, 27, 10, 6, 26, 6, 10, 27, 24, 10, 27, 21, 26, 15, 21, 26, 6, 6, 26, 6, 26, 26, 6, 9, 3, 16, 31, 6, 27, 21, 27, 21, 21, 21, 6, 29, 6, 29, 41, 26, 34, 24, 16, 1, 10, 6, 10, 26, 35, 16, 6, 24, 24, 27, 26, 21, 6, 10, 6, 27, 26, 10, 6, 10, 6, 26, 6, 26, 6, 10, 26, 26, 27, 26, 26, 24, 26, 44, 2, 13, 10, 6, 26, 10, 10, 10, 12, 38, 33, 10, 6, 10, 26, 26, 44, 6, 26, 26, 27, 26, 10, 26, 15, 29, 21, 12, 10, 31, 21, 10, 6, 24, 6, 26, 26, 6, 6, 25, 6, 26, 6, 6, 18, 29, 26, 33, 24, 10, 42, 27, 27, 27, 27, 27, 26, 27, 26, 6, 27, 24, 6, 26, 29, 26, 27, 6, 27, 6, 10, 29, 26, 10, 21, 21, 6, 27, 21, 6, 6, 21, 26, 27, 10, 26, 10, 10, 6, 21, 6, 24, 10, 41, 6, 24, 15, 26, 6, 27, 6, 27, 16, 24, 1, 10, 26, 6, 27, 27, 26, 27, 21, 27, 21, 26, 33, 16, 21, 10, 10, 26, 6, 21, 21, 10, 10, 27, 27, 10, 6, 16, 26, 6, 27, 26, 6, 24, 27, 6, 6, 15, 27, 21, 16, 26, 28, 10, 27, 21, 10, 21, 21, 6, 32, 10, 6, 19, 10, 3, 26, 6, 27, 1, 10, 26, 6, 6, 6, 24, 1, 27, 31, 21, 26, 27, 27, 6, 26, 27, 27, 26, 6, 10, 24, 27, 10, 26, 6, 26, 41, 21, 3, 15, 27, 21, 32, 6, 29, 6, 21, 26, 26, 1, 10, 26, 21, 21, 6, 27, 6, 21, 26, 29, 21, 10, 10, 6, 27, 10, 10, 10, 6, 27, 10, 6, 10, 10, 10, 6, 19, 27, 24, 27, 27, 3, 34, 27, 27, 26, 6, 6, 10, 3, 27, 10, 6, 6, 27, 6, 27, 6, 27, 21, 29, 6, 15, 27, 26, 27, 15, 10, 6, 24, 26, 21, 41, 27, 6, 24, 10, 6, 26, 27, 27, 27, 10, 10, 27, 6, 6, 10, 27, 21, 27, 26, 21, 6, 10, 10, 27, 6, 26, 6, 27, 21, 26, 6, 27, 6, 26, 26, 6, 6, 24, 24, 26, 6, 35, 26, 6, 21, 15, 6, 6, 14, 26, 29, 26, 27, 27, 27, 26, 3, 10, 2, 26, 6, 6, 6, 10, 6, 25, 29, 10, 27, 21, 26, 6, 6, 35, 24, 38, 26, 16, 6, 6, 10, 27, 26, 27, 27, 6, 10, 26, 21, 26, 6, 34, 6, 19, 6, 21, 34, 27, 27, 27, 21, 21, 10, 26, 6, 33, 29, 21, 10, 21, 26, 26, 27, 6, 27, 29, 10, 27, 6, 21, 21, 24, 41, 6, 21, 27, 27, 6, 6, 26, 35, 24, 27, 10, 21, 21, 6, 26, 21, 24, 10, 26, 6, 27, 27, 10, 34, 27, 28, 6, 6, 26, 27, 6, 29, 27, 10, 27, 26, 6, 27, 21, 26, 26, 6, 6, 26, 26, 6, 21, 33, 10, 6, 27, 12, 24, 6, 6, 10, 26, 6, 26, 31, 6, 26, 6, 6, 6, 26, 16, 27, 6, 10, 26, 16, 26, 10, 27, 26, 27, 16, 10, 26, 10, 26, 1, 27, 21, 26, 26, 6, 24, 6, 27, 2, 26, 21, 10, 21, 21, 6, 12, 12, 33, 6, 35, 26, 10, 10, 21, 25, 24, 27, 21, 6, 6, 21, 28, 26, 21, 6, 10, 6, 21, 26, 24, 21, 29, 26, 35, 6, 6, 21, 21, 24, 21, 21, 10, 6, 26, 6, 15, 6, 29, 10, 15, 26, 5, 6, 26, 6, 26, 16, 21, 26, 26, 10, 26, 6, 26, 10, 27, 19, 27, 10, 26, 27, 19, 21, 6, 10, 15, 6, 26, 6, 10, 6, 31, 10, 6, 10, 6, 21, 29, 27, 21, 24, 21, 8, 26, 6, 1, 6, 12, 12, 44, 10, 27, 3, 21, 24, 21, 33, 27, 26, 21, 6, 27, 6, 21, 20, 26, 6, 24, 26, 2, 26, 24, 26, 26, 24, 27, 26, 10, 10, 1, 21, 16, 21, 3, 21, 26, 34, 15, 10, 6, 26, 10, 6, 6, 27, 26, 26, 26, 12, 10, 24, 6, 26, 6, 13, 21, 29, 26, 10, 3, 27, 28, 6, 24, 6, 10, 24, 6, 24, 10, 27, 27, 6, 24, 24, 21, 27, 24, 6, 26, 6, 6, 21, 10, 6, 27, 6, 6, 26, 26, 26, 26, 10, 6, 33, 26, 27, 24, 10, 21, 10, 1, 6, 26, 26, 6, 26, 21, 41, 27, 27, 44, 29, 27, 10, 26, 27, 6, 27, 21, 16, 6, 6, 10, 10, 26, 26, 16, 10, 6, 9, 27, 27, 44, 34, 21, 10, 15, 26, 26, 21, 27, 26, 19, 21, 27, 21, 1, 6, 27, 10, 21, 21, 27, 27, 24, 21, 27, 7, 12, 10, 6, 26, 6, 27, 6, 6, 21, 26, 27, 27, 27, 6, 27, 27, 27, 6, 6, 26, 12, 26, 29, 27, 27, 27, 34, 26, 21, 27, 6, 26, 24, 6, 21, 26, 21, 6, 26, 3, 24, 6, 27, 27, 27, 32, 16, 26, 21, 41, 29, 6, 6, 21, 27, 6, 21, 27, 6, 6, 6, 1, 16, 10, 26, 21, 6, 27, 26, 6, 6, 26, 27, 6, 27, 26, 6, 21, 10, 6, 24, 26, 10, 26, 10, 37, 1, 24, 6, 26, 12, 26, 6, 26, 27, 33, 6, 27, 24, 26, 15, 21, 26, 34, 6, 29, 10, 26, 26, 27, 31, 6, 6, 24, 26, 6, 10, 6, 10, 25, 6, 21, 3, 24, 26, 26, 16, 24, 6, 21, 27, 32, 6, 26, 24, 10, 16, 10, 6, 26, 3, 21, 21, 6, 27, 7, 26, 26, 26, 26, 41, 6, 27, 27, 27, 18, 15, 6, 26, 31, 10, 19, 6, 16, 21, 21, 6, 27, 6, 21, 3, 6, 44, 12, 6, 10, 10, 6, 21, 10, 6, 10, 26, 26, 24, 3, 6, 27, 27, 6, 10, 6, 3, 10, 10, 3, 26, 10, 19, 27, 15, 6, 26, 27, 1, 10, 27, 10, 6, 6, 27, 10, 27, 10, 21, 6, 6, 26, 6, 10, 10, 6, 6, 21, 12, 6, 27, 32, 6, 6, 24, 27, 26, 6, 27, 21, 29, 26, 10, 21, 21, 6, 10, 6, 27, 3, 35, 6, 26, 21, 23, 6, 6, 27, 35, 18, 34, 3, 10, 21, 10, 6, 1, 33, 21, 6, 29, 33, 6, 21, 6, 10, 27, 27, 6, 6, 24, 6, 25, 24, 21, 27, 12, 10, 10, 27, 10, 26, 6, 10, 26, 35, 21, 6, 12, 6, 26, 29, 6, 6, 26, 27, 5, 27, 29, 27, 21, 27, 24, 27, 26, 26, 21, 27, 27, 32, 10, 26, 27, 26, 6, 6, 10, 6, 6, 6, 35, 26, 27, 26, 27, 26, 1, 26, 6, 26, 27, 21, 35, 1, 34, 21, 6, 6, 26, 27, 6, 31, 26, 6, 13, 27, 6, 3, 21, 27, 26, 15, 27, 16, 26, 21, 6, 27, 19, 26, 26, 35, 10, 21, 6, 27, 6, 13, 27, 35, 23, 26, 6, 21, 26, 21, 6, 10, 6, 29, 10, 1, 26, 12, 15, 6, 23, 27, 26, 26, 6, 10, 10, 26, 6, 27, 10, 27, 6, 21, 6, 6, 26, 27, 29, 27, 26, 24, 21, 24, 6, 6, 6, 27, 26, 27, 21, 3, 26, 29, 6, 26, 27, 6, 6, 6, 10, 6, 26, 6, 6, 26, 21, 5, 6, 12, 10, 27, 21, 3, 6, 21, 26, 6, 13, 26, 6, 6, 26, 6, 16, 10, 26, 26, 27, 10, 24, 21, 31, 6, 6, 6, 6, 27, 10, 24, 26, 6, 26, 34, 26, 27, 6, 21, 24, 26, 35, 10, 12, 10, 6, 26, 26, 27, 26, 26, 31, 19, 6, 24, 26, 26, 27, 27, 10, 26, 16, 6, 13, 35, 13, 6, 6, 26, 3, 15, 26, 27, 19, 24, 18, 5, 27, 27, 26, 27, 35, 6, 24, 27, 26, 6, 26, 6, 19, 33, 26, 26, 10, 26, 27, 6, 27, 6, 24, 6, 3, 41, 6, 6, 27, 10, 6, 10, 10, 21, 15, 26, 24, 6, 6, 1, 19, 27, 21, 10, 10, 24, 26, 21, 26, 26, 10, 8, 35, 27, 31, 21, 26, 6, 26, 27, 27, 26, 26, 38, 24, 27, 24, 34, 6, 27, 27, 27, 34, 27, 26, 26, 6, 21, 16, 6, 26, 26, 26, 26, 10, 24, 21, 31, 6, 26, 10, 6, 26, 26, 21, 10, 24, 21, 27, 27, 27, 26, 10, 6, 35, 6, 19, 10, 12, 26, 6, 26, 6, 35, 10, 6, 26, 29, 6, 6, 10, 26, 6, 27, 41, 10, 6, 26, 21, 26, 6, 41, 27, 15, 16, 27, 10, 26, 6, 27, 6, 27, 10, 26, 16, 6, 6, 21, 16, 6, 24, 26, 21, 21, 41, 6, 29, 10, 6, 26, 21, 26, 27, 6, 27, 26, 24, 5, 27, 6, 6, 6, 10, 15, 26, 15, 10, 21, 29, 19, 6, 21, 6, 26, 10, 6, 26, 21, 26, 6, 21, 27, 6, 29, 27, 10, 10, 6, 10, 27, 41, 10, 26, 21, 6, 10, 10, 21, 10, 26, 27, 27, 6, 16, 1, 21, 17, 26, 21, 6, 6, 26, 26, 35, 21, 31, 26, 27, 34, 15, 26, 26, 13, 44, 6, 27, 10, 6, 6, 27, 6, 6, 33, 41, 27, 21, 6, 26, 10, 27, 26, 21, 6, 26, 18, 10, 27, 27, 26, 27, 26, 27, 26, 26, 10, 6, 26, 26, 3, 26, 27, 26, 21, 21, 27, 27, 21, 35, 10, 6, 27, 21, 15, 27, 26, 26, 24, 10, 6, 21, 26, 24, 26, 6, 31, 21, 27, 10, 26, 6, 21, 27, 24, 27, 6, 10, 6, 26, 6, 10, 27, 44, 24, 26, 6, 27, 9, 10, 10, 10, 6, 26, 6, 21, 15, 29, 31, 10, 21, 26, 26, 6, 10, 10, 6, 26, 10, 25, 24, 10, 10, 29, 28, 21, 21, 26, 6, 41, 6, 27, 6, 27, 24, 24, 24, 10, 6, 10, 6, 26, 24, 21, 29, 6, 6, 21, 10, 26, 27, 26, 10, 6, 26, 27, 26, 10, 3, 10, 29, 27, 21, 26, 6, 27, 26, 6, 6, 14, 27, 27, 27, 32, 10, 21, 6, 6, 6, 21, 10, 6, 21, 14, 6, 27, 26, 6, 3, 13, 24, 27, 6, 24, 31, 26, 16, 21, 27, 6, 27, 25, 10, 27, 35, 24, 6, 6, 27, 24, 26, 6, 21, 6, 27, 27, 6, 21, 17, 26, 26, 15, 10, 26, 26, 10, 6, 41, 13, 10, 6, 10, 24, 26, 6, 26, 27, 10, 6, 16, 6, 10, 10, 10, 26, 31, 27, 6, 6, 26, 10, 26, 10, 29, 6, 21, 21, 27, 26, 25, 26, 27, 26, 6, 6, 26, 27, 10, 24, 6, 6, 6, 24, 27, 10, 1, 6, 10, 3, 6, 21, 26, 27, 6, 16, 26, 27, 10, 6, 27, 3, 5, 6, 27, 26, 26, 27, 6, 21, 26, 12, 28, 26, 3, 6, 29, 6, 6, 21, 26, 6, 26, 26, 27, 10, 6, 27, 26, 26, 31, 15, 16, 24, 29, 24, 6, 21, 6, 26, 27, 23, 29, 26, 10, 27, 6, 21, 24, 10, 6, 24, 27, 10, 6, 35, 34, 27, 27, 6, 26, 26, 27, 6, 6, 16, 5, 10, 10, 15, 1, 26, 6, 26, 26, 27, 21, 6, 29, 27, 21, 6, 27, 2, 10, 6, 26, 27, 10, 6, 33, 26, 6, 16, 10, 24, 29, 6, 27, 26, 6, 6, 10, 10, 21, 41, 26, 26, 21, 21, 26, 3, 21, 27, 27, 24, 26, 10, 6, 29, 31, 10, 21, 1, 26, 21, 29, 35, 27, 6, 10, 6, 26, 21, 10, 24, 10, 12, 26, 21, 24, 24, 6, 27, 21, 27, 27, 27, 10, 6, 16, 6, 24, 25, 27, 26, 27, 6, 1, 26, 26, 21, 10, 10, 27, 6, 27, 10, 6, 24, 15, 29, 10, 6, 6, 6, 27, 6, 26, 10, 27, 27, 6, 34, 21, 21, 27, 26, 27, 33, 2, 10, 26, 16, 26, 26, 24, 21, 6, 6, 6, 26, 6, 26, 26, 15, 26, 44, 33, 35, 26, 27, 10, 26, 21, 21, 6, 31, 26, 24, 21, 28, 32, 10, 26, 27, 21, 6, 6, 10, 27, 27, 35, 6, 6, 12, 27, 31, 21, 26, 6, 26, 16, 10, 21, 3, 26, 6, 27, 21, 21, 24, 21, 24, 21, 27, 6, 24, 10, 6, 24, 6, 6, 16, 21, 32, 29, 6, 26, 1, 6, 33, 27, 26, 33, 3, 27, 6, 27, 26, 26, 6, 26, 27, 24, 27, 42, 6, 10, 27, 21, 21, 21, 10, 26, 6, 27, 27, 21, 27, 27, 6, 23, 27, 6, 16, 10, 6, 16, 3, 26, 3, 6, 10, 10, 31, 24, 21, 6, 27, 10, 27, 26, 27, 16, 26, 26, 10, 26, 44, 27, 6, 19, 6, 6, 10, 6, 37, 32, 6, 21, 31, 6, 29, 21, 3, 27, 12, 1, 27, 15, 26, 27, 3, 26, 26, 26, 21, 26, 6, 21, 27, 19, 28, 24, 41, 27, 6, 27, 35, 27, 27, 29, 10, 26, 21, 26, 21, 26, 6, 29, 10, 27, 25, 6, 21, 6, 27, 38, 21, 29, 6, 26, 32, 6, 24, 27, 9, 6, 26, 29, 26, 27, 29, 24, 27, 10, 26, 1, 6, 26, 26, 6, 26, 6, 21, 6, 26, 6, 26, 26, 16, 6, 35, 6, 26, 10, 10, 6, 10, 6, 13, 2, 26, 26, 21, 26, 21, 31, 10, 41, 14, 27, 15, 21, 12, 10, 27, 6, 6, 27, 21, 6, 24, 10, 27, 26, 21, 26, 6, 26, 26, 6, 27, 26, 26, 44, 21, 15, 6, 24, 34, 26, 26, 6, 26, 18, 27, 15, 27, 10, 27, 21, 21, 5, 10, 6, 21, 6, 6, 6, 6, 3, 24, 21, 6, 6, 24, 24, 35, 6, 27, 10, 26, 15, 26, 6, 35, 21, 27, 27, 33, 35, 31, 6, 16, 27, 10, 26, 26, 31, 1, 6, 10, 21, 21, 13, 10, 24, 6, 35, 27, 21, 26, 12, 21, 10, 10, 24, 10, 12, 6, 10, 26, 10, 26, 15, 43, 26, 27, 6, 24, 18, 27, 34, 27, 27, 44, 26, 6, 6, 26, 21, 26, 26, 35, 6, 26, 24, 26, 21, 24, 6, 27, 6, 6, 27, 35, 27, 6, 6, 27, 6, 6, 21, 27, 21, 21, 16, 27, 27, 26, 27, 16, 27, 6, 26, 27, 33, 6, 28, 6, 29, 10, 24, 12, 26, 27, 6, 6, 6, 26, 6, 26, 41, 6, 24, 6, 1, 13, 10, 21, 6, 27, 26, 35, 26, 26, 26, 27, 16, 27, 26, 27, 6, 27, 29, 16, 21, 24, 24, 33, 29, 6, 24, 1, 26, 26, 35, 24, 27, 6, 10, 10, 26, 27, 6, 10, 16, 6, 27, 6, 26, 26, 24, 26, 10, 41, 26, 24, 26, 21, 6, 6, 6, 6, 24, 26, 6, 27, 10, 26, 6, 6, 27, 27, 6, 27, 6, 26, 10, 44, 21, 16, 26, 27, 21, 6, 26, 26, 27, 44, 26, 26, 27, 27, 26, 21, 27, 24, 21, 27, 10, 3, 34, 21, 27, 29, 26, 10, 6, 26, 31, 1, 33, 27, 10, 26, 6, 24, 34, 21, 27, 26, 21, 6, 6, 15, 10, 19, 26, 6, 10, 31, 27, 6, 6, 26, 10, 6, 6, 21, 1, 31, 1, 21, 24, 10, 26, 24, 29, 6, 27, 6, 26, 6, 6, 33, 26, 3, 29, 26, 10, 6, 10, 15, 6, 26, 6, 35, 27, 26, 21, 12, 10, 10, 21, 6, 10, 10, 27, 27, 26, 23, 10, 26, 19, 10, 21, 27, 21, 26, 3, 10, 24, 6, 24, 26, 21, 6, 10, 6, 19, 27, 3, 35, 6, 26, 6, 24, 26, 26, 26, 26, 26, 26, 44, 26, 28, 10, 35, 21, 35, 14, 27, 6, 6, 6, 23, 10, 27, 26, 6, 26, 27, 10, 28, 6, 6, 21, 6, 6, 6, 26, 6, 6, 26, 29, 6, 12, 6, 16, 15, 16, 6, 4, 28, 26, 10, 28, 6, 26, 6, 26, 26, 26, 27, 26, 10, 26, 21, 26, 10, 10, 21, 26, 6, 6, 26, 26, 27, 6, 6, 10, 27, 26, 6, 10, 26, 6, 27, 27, 10, 29, 14, 10, 15, 6, 26, 26, 6, 6, 15, 27, 6, 27, 6, 10, 16, 6, 41, 6, 6, 6, 26, 27, 24, 21, 26, 6, 26, 24, 34, 16, 15, 15, 24, 6, 10, 6, 26, 6, 6, 6, 1, 23, 24, 6, 6, 27, 24, 24, 26, 27, 21, 24, 26, 29, 26, 41, 6, 26, 21, 26, 26, 26, 34, 10, 26, 27, 26, 21, 21, 27, 6, 27, 10, 24, 6, 26, 16, 21, 23, 6, 16, 27, 27, 21, 26, 19, 26, 29, 24, 21, 26, 6, 27, 29, 27, 14, 9, 10, 10, 21, 26, 10, 26, 10, 12, 6, 26, 10, 27, 31, 35, 17, 6, 26, 10, 10, 26, 26, 25, 6, 13, 21, 2, 44, 10, 26, 21, 19, 27, 10, 6, 27, 35, 24, 27, 6, 27, 10, 27, 21, 10, 31, 21, 26, 10, 27, 6, 10, 21, 6, 26, 6, 26, 6, 29, 6, 21, 26, 24, 14, 33, 27, 29, 15, 27, 27, 21, 10, 6, 6, 16, 34, 12, 6, 6, 26, 6, 6, 1, 37, 24, 6, 6, 6, 6, 27, 6, 27, 16, 6, 6, 27, 27, 26, 21, 26, 12, 6, 6, 41, 26, 33, 26, 6, 27, 6, 26, 21, 6, 10, 10, 35, 6, 10, 27, 27, 24, 21, 27, 35, 6, 6, 27, 27, 21, 6, 26, 6, 29, 10, 35, 6, 6, 27, 10, 6, 26, 6, 6, 21, 21, 6, 6, 10, 21, 27, 15, 5, 10, 10, 6, 27, 12, 15, 15, 26, 6, 27, 21, 26, 21, 26, 26, 21, 21, 26, 18, 10, 21, 6, 26, 10, 6, 27, 26, 27, 6, 6, 26, 26, 24, 6, 21, 27, 26, 21, 18, 14, 6, 27, 15, 10, 26, 27, 6, 35, 26, 21, 6, 26, 6, 27, 26, 6, 10, 6, 10, 6, 26, 24, 26, 6, 42, 6, 27, 26, 6, 29, 3, 6, 6, 21, 21, 21, 6, 6, 27, 6, 26, 27, 10, 21, 31, 10, 15, 37, 6, 21, 5, 6, 6, 6, 44, 1, 21, 10, 21, 15, 10, 6, 6, 27, 24, 10, 6, 6, 21, 6, 6, 29, 6, 6, 26, 6, 27, 6, 6, 27, 27, 13, 27, 15, 6, 10, 6, 27, 27, 28, 26, 6, 31, 10, 27, 26, 26, 6, 10, 6, 6, 26, 27, 27, 6, 21, 35, 26, 26, 26, 27, 26, 26, 6, 10, 24, 24, 6, 12, 26, 27, 26, 27, 6, 27, 21, 10, 26, 31, 9, 27, 6, 28, 1, 6, 6, 6, 21, 38, 31, 23, 6, 1, 21, 6, 6, 10, 10, 21, 26, 10, 25, 6, 27, 27, 6, 29, 26, 27, 27, 26, 21, 27, 13, 5, 27, 31, 24, 21, 3, 34, 10, 26, 21, 10, 6, 6, 21, 27, 6, 26, 27, 24, 29, 26, 3, 26, 21, 14, 1, 6, 27, 26, 27, 26, 6, 3, 6, 24, 27, 6, 27, 21, 27, 12, 10, 10, 27, 26, 10, 6, 27, 24, 16, 29, 32, 26, 6, 6, 24, 6, 10, 27, 10, 26, 26, 21, 6, 21, 15, 6, 6, 16, 3, 6, 31, 33, 6, 6, 24, 16, 21, 24, 27, 6, 42, 27, 26, 26, 27, 3, 26, 6, 27, 26, 29, 21, 21, 27, 26, 6, 6, 27, 6, 21, 26, 10, 26, 26, 10, 21, 14, 27, 26, 6, 10, 26, 26, 27, 33, 6, 26, 27, 16, 31, 21, 5, 26, 26, 35, 26, 21, 6, 16, 27, 6, 27, 21, 10, 6, 27, 24, 26, 26, 27, 6, 10, 38, 33, 31, 26, 6, 24, 10, 34, 6, 1, 10, 10, 6, 10, 10, 21, 27, 21, 10, 10, 3, 31, 14, 24, 6, 31, 21, 35, 12, 3, 6, 6, 6, 6, 27, 6, 10, 29, 44, 10, 10, 16, 41, 5, 24, 31, 6, 6, 27, 10, 16, 26, 10, 26, 26, 24, 6, 42, 6, 16, 10, 24, 21, 26, 10, 26, 21, 13, 26, 27, 1, 21, 27, 6, 24, 24, 6, 6, 41, 12, 21, 26, 18, 26, 19, 10, 6, 6, 10, 24, 21, 31, 10, 27, 10, 26, 6, 16, 26, 12, 14, 10, 6, 21, 6, 6, 21, 1, 10, 21, 10, 6, 27, 10, 6, 26, 6, 10, 21, 26, 24, 27, 10, 13, 16, 24, 6, 6, 27, 6, 6, 6, 38, 31, 6, 24, 1, 29, 26, 16, 21, 26, 10, 6, 27, 6, 27, 9, 21, 6, 10, 24, 16, 26, 27, 23, 10, 32, 27, 6, 16, 24, 10, 6, 10, 6, 26, 14, 6, 10, 24, 27, 26, 21, 29, 29, 26, 10, 44, 26, 10, 26, 21, 21, 3, 27, 26, 10, 44, 29, 6, 3, 6, 3, 12, 26, 6, 26, 15, 6, 6, 10, 27, 6, 29, 21, 26, 6, 10, 6, 3, 6, 27, 10, 21, 6, 6, 26, 27, 27, 33, 21, 6, 13, 26, 6, 16, 29, 6, 6, 8, 27, 21, 6, 33, 3, 6, 24, 27, 15, 15, 6, 26, 29, 24, 21, 6, 24, 6, 6, 27, 6, 26, 21, 10, 6, 29, 6, 10, 10, 6, 6, 6, 26, 10, 12, 21, 10, 24, 6, 16, 21, 21, 27, 10, 6, 6, 26, 10, 6, 26, 27, 21, 21, 26, 21, 6, 26, 10, 26, 21, 26, 6, 27, 10, 26, 6, 6, 10, 10, 6, 6, 44, 26, 10, 21, 26, 21, 26, 16, 21, 27, 21, 26, 27, 10, 27, 26, 27, 26, 10, 24, 14, 6, 26, 21, 6, 16, 26, 27, 6, 10, 6, 26, 26, 6, 26, 6, 21, 27, 27, 27, 26, 6, 26, 27, 26, 10, 10, 10, 6, 6, 14, 24, 6, 35, 10, 26, 28, 27, 6, 21, 10, 6, 27, 6, 26, 6, 21, 21, 26, 27, 34, 10, 10, 6, 10, 35, 26, 21, 27, 31, 10, 16, 24, 27, 26, 10, 6, 6, 26, 27, 21, 6, 10, 27, 29, 27, 6, 21, 3, 26, 27, 21, 26, 27, 6, 26, 27, 26, 6, 26, 26, 24, 6, 27, 27, 26, 3, 3, 27, 6, 24, 10, 26, 6, 6, 26, 26, 27, 29, 6, 6, 26, 1, 6, 26, 35, 10, 6, 27, 21, 9, 27, 26, 27, 21, 1, 6, 6, 27, 26, 6, 21, 26, 6, 6, 26, 13, 26, 44, 27, 25, 3, 21, 33, 10, 26, 26, 27, 21, 26, 21, 26, 6, 21, 27, 10, 6, 21, 1, 6, 10, 6, 26, 6, 16, 6, 26, 15, 27, 21, 6, 6, 27, 32, 26, 6, 21, 10, 10, 16, 26, 26, 21, 26, 6, 21, 6, 3, 27, 21, 26, 26, 26, 24, 27, 16, 26, 27, 27, 27, 26, 31, 6, 35, 6, 29, 26, 27, 3, 27, 10, 21, 6, 6, 10, 24, 26, 6, 6, 27, 21, 6, 21, 21, 3, 10, 27, 27, 27, 26, 26, 21, 27, 26, 27, 27, 27, 27, 10, 21, 26, 2, 6, 6, 27, 26, 27, 16, 12, 41, 26, 10, 10, 24, 27, 27, 26, 13, 6, 6, 28, 27, 27, 34, 29, 6, 27, 26, 21, 16, 10, 26, 3, 10, 12, 24, 25, 27, 6, 35, 29, 27, 6, 6, 27, 21, 27, 16, 6, 26, 21, 26, 27, 10, 6, 6, 32, 27, 15, 21, 27, 27, 6, 6, 27, 41, 26, 6, 6, 6, 21, 6, 6, 10, 27, 27, 6, 10, 6, 21, 6, 13, 21, 6, 26, 26, 21, 26, 26, 6, 21, 27, 27, 26, 3, 6, 24, 6, 6, 27, 6, 10, 26, 10, 6, 26, 21, 6, 24, 27, 6, 16, 33, 10, 26, 26, 27, 6, 21, 10, 35, 10, 10, 27, 25, 6, 27, 26, 26, 24, 21, 44, 27, 6, 10, 27, 6, 10, 27, 5, 31, 26, 26, 3, 21, 15, 21, 24, 27, 21, 26, 18, 6, 26, 26, 27, 27, 6, 16, 10, 26, 27, 19, 6, 6, 21, 6, 6, 26, 26, 27, 27, 6, 26, 26, 6, 6, 10, 24, 3, 27, 16, 26, 27, 26, 27, 6, 27, 24, 6, 41, 27, 23, 12, 13, 44, 16, 26, 35, 26, 6, 10, 26, 6, 21, 21, 27, 26, 10, 15, 26, 29, 27, 16, 21, 26, 10, 6, 6, 23, 35, 6, 12, 27, 6, 27, 16, 27, 16, 26, 21, 26, 6, 26, 10, 27, 6, 26, 33, 10, 26, 6, 26, 6, 33, 10, 19, 13, 29, 6, 23, 26, 26, 12, 27, 29, 6, 21, 26, 2, 26, 26, 27, 24, 14, 27, 21, 21, 26, 27, 21, 26, 27, 6, 21, 6, 6, 6, 26, 29, 27, 21, 8, 6, 21, 24, 24, 26, 10, 6, 21, 10, 26, 26, 27, 27, 27, 10, 27, 3, 3, 27, 26, 41, 27, 10, 24, 27, 24, 1, 37, 24, 27, 26, 6, 10, 26, 6, 21, 26, 27, 35, 27, 27, 6, 21, 13, 10, 26, 27, 21, 6, 10, 6, 27, 10, 2, 6, 26, 10, 27, 6, 10, 6, 6, 16, 24, 26, 27, 29, 6, 2, 26, 26, 6, 26, 21, 26, 44, 27, 26, 27, 27, 6, 27, 21, 26, 6, 10, 3, 6, 35, 21, 26, 6, 6, 26, 10, 35, 27, 1, 6, 26, 6, 6, 6, 3, 6, 27, 26, 6, 21, 6, 24, 12, 6, 21, 26, 26, 6, 6, 6, 6, 27, 10, 24, 6, 6, 21, 27, 24, 6, 6, 24, 21, 6, 21, 24, 27, 26, 3, 21, 24, 26, 10, 24, 6, 27, 6, 10, 10, 21, 24, 27, 26, 33, 10, 10, 24, 26, 27, 21, 27, 26, 21, 6, 10, 23, 41, 27, 34, 27, 15, 15, 18, 10, 21, 27, 5, 27, 28, 24, 6, 6, 27, 21, 23, 1, 21, 6, 6, 10, 6, 21, 6, 24, 6, 6, 26, 6, 26, 10, 6, 41, 27, 6, 27, 21, 27, 12, 24, 26, 27, 26, 10, 6, 10, 27, 6, 26, 35, 27, 24, 21, 6, 6, 33, 21, 6, 27, 1, 35, 26, 6, 15, 27, 3, 21, 6, 33, 16, 21, 6, 10, 21, 6, 6, 26, 26, 26, 26, 6, 6, 27, 26, 6, 27, 21, 26, 10, 21, 24, 6, 24, 26, 6, 26, 26, 26, 27, 6, 10, 34, 10, 12, 6, 26, 26, 27, 10, 35, 6, 6, 26, 6, 14, 35, 13, 27, 35, 24, 31, 2, 27, 27, 6, 10, 26, 10, 6, 10, 21, 32, 6, 44, 6, 26, 10, 26, 27, 6, 26, 27, 26, 6, 38, 6, 10, 24, 26, 26, 10, 12, 24, 19, 26, 21, 21, 26, 24, 26, 21, 6, 26, 1, 27, 21, 14, 21, 6, 21, 6, 6, 21, 27, 21, 6, 26, 6, 6, 26, 1, 6, 6, 25, 27, 6, 27, 3, 24, 24, 26, 6, 27, 6, 27, 6, 24, 26, 10, 26, 1, 6, 10, 26, 27, 24, 10, 24, 6, 27, 27, 26, 14, 35, 26, 6, 26, 27, 26, 6, 21, 21, 18, 18, 21, 6, 29, 13, 6, 38, 10, 33, 33, 10, 26, 6, 41, 27, 16, 6, 35, 6, 27, 6, 26, 10, 27, 10, 26, 6, 6, 6, 32, 24, 27, 24, 35, 27, 6, 27, 27, 6, 26, 10, 27, 6, 21, 16, 10, 18, 26, 27, 26, 21, 27, 6, 26, 27, 29, 26, 10, 0, 10, 16, 29, 15, 26, 19, 29, 26, 6, 26, 26, 24, 27, 21, 10, 35, 10, 6, 10, 6, 34, 10, 27, 10, 6, 25, 24, 23, 26, 6, 6, 6, 26, 27, 26, 27, 27, 16, 24, 16, 10, 26, 10, 21, 10, 26, 10, 6, 6, 27, 21, 10, 27, 27, 26, 6, 27, 41, 16, 26, 26, 6, 27, 6, 6, 6, 27, 24, 10, 27, 6, 25, 6, 31, 26, 10, 6, 31, 32, 10, 19, 21, 3, 26, 6, 27, 27, 10, 26, 27, 6, 6, 10, 27, 18, 6, 6, 16, 21, 27, 10, 24, 24, 27, 27, 26, 15, 24, 27, 27, 21, 10, 26, 26, 27, 21, 27, 2, 10, 10, 21, 6, 21, 10, 10, 27, 29, 21, 21, 26, 27, 26, 26, 6, 31, 29, 6, 26, 15, 21, 26, 27, 27, 6, 27, 10, 27, 26, 6, 10, 27, 27, 6, 37, 21, 6, 26, 14, 1, 6, 26, 27, 6, 26, 21, 21, 29, 41, 15, 24, 27, 10, 10, 10, 26, 6, 6, 10, 26, 26, 6, 26, 3, 41, 27, 6, 21, 10, 6, 26, 21, 6, 29, 21, 10, 34, 24, 10, 27, 29, 27, 26, 27, 26, 6, 24, 27, 6, 21, 35, 6, 26, 24, 26, 10, 27, 6, 6, 31, 35, 10, 16, 6, 16, 6, 27, 27, 6, 26, 31, 21, 16, 27, 26, 26, 27, 6, 26, 48, 9, 27, 21, 27, 19, 27, 27, 16, 10, 27, 6, 27, 6, 15, 27, 24, 26, 26, 21, 24, 26, 26, 6, 24, 6, 27, 27, 6, 34, 26, 3, 21, 26, 6, 6, 6, 27, 15, 6, 24, 24, 27, 26, 31, 26, 6, 26, 10, 24, 21, 26, 6, 24, 27, 26, 21, 21, 27, 24, 44, 24, 6, 27, 27, 26, 6, 21, 10, 27, 6, 26, 24, 24, 23, 18, 35, 26, 10, 21, 27, 26, 27, 21, 26, 21, 26, 6, 26, 6, 1, 27, 21, 6, 15, 21, 10, 24, 26, 10, 1, 6, 10, 26, 26, 6, 26, 26, 27, 32, 6, 5, 10, 21, 21, 19, 26, 31, 27, 10, 26, 26, 6, 10, 24, 15, 6, 3, 21, 26, 21, 6, 26, 10, 34, 6, 6, 21, 16, 27, 12, 10, 15, 26, 27, 7, 10, 6, 1, 27, 10, 26, 6, 10, 27, 27, 26, 26, 6, 27, 6, 26, 27, 6, 27, 27, 10, 26, 10, 23, 26, 27, 27, 10, 15, 10, 10, 10, 6, 6, 24, 27, 6, 6, 21, 3, 6, 21, 16, 24, 26, 10, 6, 15, 26, 21, 24, 27, 6, 6, 26, 27, 25, 6, 27, 6, 21, 26, 26, 10, 27, 21, 6, 27, 27, 6, 10, 35, 26, 35, 27, 27, 27, 6, 34, 6, 6, 6, 26, 26, 29, 26, 27, 10, 29, 24, 6, 10, 6, 6, 27, 26, 26, 24, 10, 33, 26, 21, 21, 18, 24, 10, 6, 27, 6, 26, 26, 27, 6, 10, 13, 24, 10, 24, 26, 6, 10, 19, 25, 10, 10, 6, 6, 26, 26, 5, 6, 26, 6, 26, 6, 27, 15, 10, 26, 27, 27, 26, 26, 24, 15, 26, 26, 27, 6, 21, 6, 10, 24, 24, 26, 27, 27, 27, 10, 21, 26, 10, 6, 26, 10, 27, 21, 24, 27, 24, 26, 6, 26, 27, 24, 10, 6, 6, 27, 48, 21, 26, 35, 27, 15, 10, 27, 27, 27, 27, 16, 27, 24, 27, 26, 34, 13, 21, 26, 10, 10, 35, 44, 21, 33, 10, 10, 6, 10, 6, 26, 6, 10, 26, 16, 6, 10, 29, 6, 10, 26, 26, 27, 13, 34, 21, 21, 27, 24, 21, 6, 16, 26, 6, 35, 6, 13, 27, 6, 1, 26, 6, 27, 6, 21, 6, 27, 1, 26, 26, 10, 6, 31, 26, 6, 31, 10, 6, 6, 33, 24, 26, 27, 6, 27, 24, 10, 24, 10, 10, 26, 3, 29, 6, 6, 6, 6, 6, 10, 26, 27, 21, 18, 28, 6, 27, 35, 6, 23, 26, 12, 9, 6, 26, 26, 10, 6, 26, 10, 6, 29, 26, 21, 21, 10, 1, 6, 26, 6, 6, 21, 21, 33, 13, 10, 6, 16, 10, 26, 26, 24, 10, 24, 10, 6, 12, 27, 18, 6, 10, 27, 1, 26, 10, 24, 27, 21, 27, 6, 26, 26, 26, 24, 26, 26, 10, 6, 24, 6, 10, 27, 21, 6, 10, 21, 26, 21, 6, 33, 26, 6, 16, 6, 21, 35, 1, 26, 6, 6, 15, 27, 6, 6, 21, 27, 15, 32, 10, 10, 27, 26, 6, 1, 26, 23, 33, 21, 27, 26, 6, 10, 35, 10, 6, 27, 6, 16, 26, 10, 6, 13, 44, 26, 27, 21, 21, 27, 26, 24, 27, 26, 6, 26, 1, 3, 26, 21, 6, 6, 6, 24, 26, 24, 10, 10, 10, 6, 6, 26, 27, 26, 27, 26, 44, 27, 26, 10, 10, 10, 26, 6, 6, 16, 27, 6, 24, 18, 10, 6, 10, 10, 10, 26, 6, 21, 24, 32, 6, 6, 24, 1, 27, 6, 27, 21, 26, 26, 42, 6, 26, 6, 6, 27, 6, 10, 24, 1, 10, 27, 6, 26, 21, 21, 26, 6, 26, 26, 1, 10, 6, 10, 26, 26, 25, 6, 27, 16, 32, 27, 26, 10, 21, 26, 26, 5, 24, 26, 27, 13, 37, 24, 24, 29, 27, 10, 6, 41, 27, 6, 21, 10, 27, 6, 26, 21, 26, 27, 1, 24, 6, 27, 26, 16, 26, 26, 6, 10, 3, 41, 10, 21, 27, 27, 26, 6, 24, 24, 31, 21, 2, 26, 27, 10, 10, 6, 27, 21, 10, 10, 27, 35, 6, 16, 3, 16, 10, 6, 24, 21, 6, 24, 26, 44, 6, 26, 26, 6, 10, 26, 26, 21, 27, 21, 34, 24, 12, 10, 1, 26, 10, 24, 6, 6, 33, 27, 45, 16, 26, 10, 26, 6, 6, 10, 29, 27, 6, 6, 27, 21, 26, 10, 10, 26, 24, 35, 6, 26, 6, 21, 24, 10, 21, 6, 27, 6, 27, 1, 27, 21, 6, 27, 21, 26, 10, 1, 3, 6, 26, 28, 26, 26, 34, 6, 27, 21, 6, 6, 6, 6, 24, 34, 29, 27, 6, 16, 26, 26, 6, 21, 18, 26, 27, 27, 24, 10, 27, 6, 26, 24, 27, 10, 6, 10, 6, 16, 27, 6, 21, 26, 21, 27, 26, 16, 27, 1, 6, 21, 26, 1, 31, 27, 27, 26, 6, 6, 10, 10, 21, 12, 6, 6, 21, 6, 28, 6, 6, 29, 6, 29, 24, 26, 6, 27, 24, 21, 21, 6, 10, 10, 26, 26, 10, 26, 6, 10, 27, 3, 27, 21, 10, 6, 27, 35, 26, 29, 10, 6, 27, 26, 21, 26, 6, 6, 10, 16, 10, 6, 6, 26, 6, 24, 10, 27, 24, 26, 27, 6, 27, 6, 15, 35, 6, 3, 26, 26, 27, 24, 6, 27, 19, 27, 27, 27, 26, 19, 14, 6, 6, 24, 28, 27, 12, 41, 26, 10, 26, 6, 27, 29, 6, 27, 21, 27, 6, 27, 6, 27, 10, 26, 26, 6, 27, 24, 3, 19, 27, 6, 34, 21, 21, 21, 27, 21, 27, 6, 21, 35, 6, 6, 27, 6, 6, 6, 27, 21, 21, 21, 27, 13, 6, 21, 6, 10, 35, 26, 12, 6, 21, 6, 21, 10, 26, 26, 6, 6, 6, 27, 27, 27, 27, 26, 26, 27, 26, 27, 31, 26, 26, 6, 6, 6, 6, 26, 26, 27, 27, 6, 6, 27, 10, 27, 26, 10, 15, 6, 24, 26, 21, 6, 27, 21, 26, 27, 27, 3, 24, 26, 32, 10, 6, 29, 27, 6, 6, 6, 8, 27, 6, 27, 6, 1, 26, 6, 26, 33, 21, 26, 6, 27, 35, 6, 15, 6, 10, 6, 21, 26, 37, 26, 10, 26, 14, 31, 32, 24, 27, 26, 27, 18, 24, 35, 23, 1, 6, 31, 26, 29, 6, 27, 13, 41, 10, 26, 21, 10, 27, 31, 6, 26, 6, 26, 12, 26, 27, 13, 26, 21, 26, 26, 6, 21, 41, 15, 6, 6, 27, 26, 6, 26, 33, 27, 25, 7, 26, 27, 21, 6, 26, 27, 6, 26, 12, 15, 21, 26, 26, 10, 6, 26, 33, 27, 3, 35, 26, 6, 27, 10, 6, 27, 26, 10, 27, 6, 6, 21, 24, 13, 27, 41, 6, 27, 26, 6, 27, 26, 21, 29, 6, 21, 10, 6, 24, 26, 27, 27, 26, 6, 21, 21, 27, 35, 6, 6, 10, 21, 27, 21, 26, 6, 6, 27, 21, 6, 6, 6, 21, 21, 6, 10, 6, 26, 21, 26, 26, 26, 10, 24, 26, 15, 6, 6, 6, 6, 26, 35, 26, 24, 29, 15, 21, 26, 26, 27, 6, 27, 6, 10, 26, 27, 6, 27, 31, 21, 26, 12, 27, 15, 21, 27, 27, 21, 27, 3, 6, 6, 6, 6]\n"
     ]
    }
   ],
   "source": [
    "print(model_preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "id": "2ccad464",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 'Diego Velazquez',\n",
       " 1: 'Vincent van Gogh',\n",
       " 2: 'Claude Monet',\n",
       " 3: 'Edgar Degas',\n",
       " 4: 'Hieronymus Bosch',\n",
       " 5: 'Pierre-Auguste Renoir',\n",
       " 6: 'Rene Magritte',\n",
       " 7: 'Michelangelo',\n",
       " 8: 'Peter Paul Rubens',\n",
       " 9: 'Caravaggio',\n",
       " 10: 'Alfred Sisley',\n",
       " 11: 'Edouard Manet',\n",
       " 12: 'Rembrandt',\n",
       " 13: 'Francisco Goya',\n",
       " 14: 'Pablo Picasso',\n",
       " 15: 'Titian',\n",
       " 16: 'Mikhail Vrubel',\n",
       " 17: 'Leonardo da Vinci',\n",
       " 18: 'Kazimir Malevich',\n",
       " 19: 'Andy Warhol',\n",
       " 20: 'Vasiliy Kandinskiy',\n",
       " 21: 'Gustav Klimt',\n",
       " 22: 'Amedeo Modigliani',\n",
       " 23: 'Henri Rousseau',\n",
       " 24: 'Salvador Dali',\n",
       " 25: 'Pieter Bruegel',\n",
       " 26: 'Albrecht Du rer',\n",
       " 27: 'Paul Gauguin',\n",
       " 28: 'Sandro Botticelli',\n",
       " 29: 'Piet Mondrian',\n",
       " 30: 'Eugene Delacroix',\n",
       " 31: 'Paul Klee',\n",
       " 32: 'William Turner',\n",
       " 33: 'Marc Chagall',\n",
       " 34: 'Jan van Eyck',\n",
       " 35: 'Henri Matisse',\n",
       " 36: 'El Greco',\n",
       " 37: 'Gustave Courbet',\n",
       " 38: 'Andrei Rublev',\n",
       " 39: 'Jackson Pollock',\n",
       " 40: 'Edvard Munch',\n",
       " 41: 'Camille Pissarro',\n",
       " 42: 'Raphael',\n",
       " 43: 'Henri de Toulouse-Lautrec',\n",
       " 44: 'Joan Miro',\n",
       " 45: 'Giotto di Bondone',\n",
       " 46: 'Diego Rivera',\n",
       " 47: 'Frida Kahlo',\n",
       " 48: 'Georges Seurat',\n",
       " 49: 'Paul Cezanne'}"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_dict_decode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 247,
   "id": "9b65d1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# epoch = 10, f1_score = 0.6480124118321071"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 411,
   "id": "a1a34b9b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "33"
      ]
     },
     "execution_count": 411,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(this[this['artist'] == i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 414,
   "id": "8d37f8f4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diego Velazquez : 81\n",
      "Vincent van Gogh : 629\n",
      "Claude Monet : 59\n",
      "Edgar Degas : 490\n",
      "Hieronymus Bosch : 115\n",
      "Pierre-Auguste Renoir : 233\n",
      "Rene Magritte : 137\n",
      "Michelangelo : 34\n",
      "Peter Paul Rubens : 97\n",
      "Caravaggio : 32\n",
      "Alfred Sisley : 164\n",
      "Edouard Manet : 62\n",
      "Rembrandt : 181\n",
      "Francisco Goya : 204\n",
      "Pablo Picasso : 303\n",
      "Titian : 173\n",
      "Mikhail Vrubel : 118\n",
      "Leonardo da Vinci : 101\n",
      "Kazimir Malevich : 91\n",
      "Andy Warhol : 132\n",
      "Vasiliy Kandinskiy : 60\n",
      "Gustav Klimt : 69\n",
      "Amedeo Modigliani : 132\n",
      "Henri Rousseau : 52\n",
      "Salvador Dali : 99\n",
      "Pieter Bruegel : 85\n",
      "Albrecht Du rer : 220\n",
      "Paul Gauguin : 220\n",
      "Sandro Botticelli : 120\n",
      "Piet Mondrian : 59\n",
      "Eugene Delacroix : 26\n",
      "Paul Klee : 142\n",
      "William Turner : 44\n",
      "Marc Chagall : 173\n",
      "Jan van Eyck : 64\n",
      "Henri Matisse : 121\n",
      "El Greco : 65\n",
      "Gustave Courbet : 42\n",
      "Andrei Rublev : 74\n",
      "Jackson Pollock : 21\n",
      "Edvard Munch : 44\n",
      "Camille Pissarro : 64\n",
      "Raphael : 73\n",
      "Henri de Toulouse-Lautrec : 61\n",
      "Joan Miro : 76\n",
      "Giotto di Bondone : 72\n",
      "Diego Rivera : 50\n",
      "Frida Kahlo : 84\n",
      "Georges Seurat : 30\n",
      "Paul Cezanne : 33\n"
     ]
    }
   ],
   "source": [
    " this = pd.read_csv(f'{data_dir}/train.csv')\n",
    "for i in this['artist'].unique():\n",
    "    count = len(this[this['artist'] == i])\n",
    "    print(f'{i} : {count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f160c574",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
